{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Imports and Initial Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report, precision_recall_curve\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from catboost import CatBoostClassifier\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "from lime.submodular_pick import SubmodularPick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./healthcare-dataset-stroke-data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data.select_dtypes(include=\"object\").columns :\n",
    "    data[col] = data[col].astype(\"category\")\n",
    "data = data.drop(columns=[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "| Feature             | Type               | Description                           | Possible Values / Format                                           | Explanation / Notes                                               |\n",
    "| ------------------- | ------------------ | ------------------------------------- | ------------------------------------------------------------------ | -----------------------------------\n",
    "| `gender`            | Categorical        | Biological sex of the patient         | `Male`, `Female`                                                   | Can be used as a feature; may need encoding for ML models         |\n",
    "| `age`               | Numerical          | Age of the patient in years           | Float                                                              | Important risk factor for stroke; higher age often increases risk |\n",
    "| `hypertension`      | Categorical/Binary | Whether patient has hypertension      | `0` (No), `1` (Yes)                                                | Hypertension is a key stroke risk factor                          |\n",
    "| `heart_disease`     | Categorical/Binary | Whether patient has any heart disease | `0` (No), `1` (Yes)                                                | Another major risk factor for stroke                              |\n",
    "| `ever_married`      | Categorical        | Marital status                        | `Yes`, `No`                                                        | Could correlate with lifestyle or social support                  |\n",
    "| `work_type`         | Categorical        | Type of occupation                    | `Private`, `Self-employed`, `Govt_job`, `Children`, `Never_worked` | Can reflect lifestyle and stress levels                           |\n",
    "| `Residence_type`    | Categorical        | Urban or rural living area            | `Urban`, `Rural`                                                   | Can affect access to healthcare and lifestyle patterns            |\n",
    "| `avg_glucose_level` | Numerical          | Average blood glucose level (mg/dL)   | Float                                                              | High glucose/diabetes increases stroke risk                       |\n",
    "| `bmi`               | Numerical          | Body Mass Index                       | Float (may contain NaN)                                            | Obesity is a risk factor; missing values may need imputation      |\n",
    "| `smoking_status`    | Categorical        | Smoking habits                        | `never smoked`, `formerly smoked`, `smokes`, `Unknown`             | Smoking is a major risk factor; `Unknown` needs special handling  |\n",
    "| `stroke`            | Categorical/Binary | Whether patient has had a stroke      | `0` (No), `1` (Yes)                                                | Target variable for prediction                                    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Unvalidated values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "> The minimum value of the age column is 0.008 years , which is suspicious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Distribution of Age\")\n",
    "plt.ylabel(\"Age\")\n",
    "plt.hist(data[\"age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data[\"age\"]<=1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "> ðŸ’¡ Upon examining the dataset, the small age values (e.g., 0.08, 0.16, 0.32 years) correspond to children under 1 year old.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "# Data Cleaning & Feature Engineering + Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## NaN Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Data distribution before filling Nan\")\n",
    "plt.xlabel(\"BMI\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.hist(data[\"bmi\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Fill NaN with median since the data distribution is skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"bmi\"] = data[\"bmi\"].fillna(data[\"bmi\"].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Data distribution after filling Nan\")\n",
    "plt.xlabel(\"BMI\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.hist(data[\"bmi\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Duplicated Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of duplicated rows: \", data.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Detecting Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data.drop(columns=[\"stroke\"]).select_dtypes(\"number\").columns:\n",
    "    plt.figure()  \n",
    "    plt.title(f\"Data distribution of {col}\")\n",
    "    plt.hist(data[col])  \n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = data.drop(columns=[\"hypertension\", \"heart_disease\", \"stroke\"]).select_dtypes(include=\"number\").columns\n",
    "\n",
    "for col in numeric_cols:\n",
    "    plt.figure(figsize=(6, 4))  \n",
    "    plt.title(f\"Box plot of {col}\")\n",
    "    plt.boxplot(data[col])\n",
    "    plt.xlabel(col)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "> ðŸ’¡ Although there are some outliers in bmi and avg_glucose_level, We will primarily use tree-based models (e.g., Random Forest, XGBoost, CatBoost), which are naturally robust to outliers. Therefore, these extreme values will be retained, as they may carry important information for predicting stroke."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### Data Cleaning Based on Counterfactual Analysis\n",
    "\n",
    "Upon examining the dataset and generated counterfactuals, we identified **two types of clinically implausible cases**:\n",
    "\n",
    "1. **Extremely low glucose levels** â€“ Some counterfactuals suggested that lowering glucose would increase stroke risk. This is **not supported by medical evidence** and occurs only in a small subset of patients (rare outliers).  \n",
    "\n",
    "2. **Extremely high BMI values (â‰¥ 50)** â€“ A few instances indicated that very high BMI decreases stroke risk, which is **clinically unrealistic** and also represents a small portion of the dataset.  \n",
    "\n",
    "Given that these cases are rare and potentially misleading, it is reasonable to remove them to **improve the plausibility and reliability** of subsequent analyses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where avg_glucose_level <= 70 AND stroke == 1\n",
    "data = data[~((data['avg_glucose_level'] <= 70) & (data['stroke'] == 1))]\n",
    "data = data[~(data['bmi'] >= 50)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Imbalanced target classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Data distribution of stroke\")\n",
    "plt.hist(data[\"stroke\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### So the target value is highly imbalanced. I will using SMOTE to oversampling the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select_dtypes(\"number\").corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "# Building models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_mappings = {}\n",
    "dataForModel = data.copy()\n",
    "for col in ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']:\n",
    "    dataForModel[col] = dataForModel[col].astype('category')\n",
    "    category_mappings[col] = dict(enumerate(dataForModel[col].cat.categories))\n",
    "    dataForModel[col] = dataForModel[col].cat.codes\n",
    "\n",
    "X = dataForModel.drop(columns=\"stroke\")\n",
    "y = dataForModel[\"stroke\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "plt.hist(y_train_res)\n",
    "plt.title(\"Target classes after using SMOTE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "### Function to calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(y_true, y_pred, y_proba):\n",
    "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(\"Precision:\", precision_score(y_true, y_pred))\n",
    "    print(\"Recall:\", recall_score(y_true, y_pred))\n",
    "    print(\"F1-Score:\", f1_score(y_true, y_pred))\n",
    "    print(\"ROC-AUC:\", roc_auc_score(y_true, y_proba))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(cm)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sn.heatmap(\n",
    "        cm, \n",
    "        annot=True, \n",
    "        fmt=\"d\", \n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=[\"Predicted No Stroke\", \"Predicted Stroke\"],\n",
    "        yticklabels=[\"Actual No Stroke\", \"Actual Stroke\"]\n",
    "    )\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "## Random forest "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "#### Tunning hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 500],      # number of trees\n",
    "    'max_depth': [None, 5, 10, 20],      # max depth of tree\n",
    "    'min_samples_split': [2, 5, 10],     # min samples to split\n",
    "    'min_samples_leaf': [1, 2, 4],       # min samples per leaf\n",
    "    'class_weight': ['balanced', {0:1,1:5}, {0:1,1:10}]  # emphasize minority\n",
    "}\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    scoring='recall',  # maximize recall for minority class\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "\n",
    "grid_search.fit(X_train_res, y_train_res)\n",
    "\n",
    "print(\"Best parameters found:\", grid_search.best_params_)\n",
    "print(\"Best recall score:\", grid_search.best_score_)\n",
    "\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "y_proba_rf = best_rf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(\"===== Random Forest (Grid Search) =====\")\n",
    "evaluate_model(y_test, y_pred_rf, y_proba_rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "#### Using tunned hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=5,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=1,\n",
    "    class_weight={0: 1, 1: 2},  \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "rf.fit(X_train_res, y_train_res)\n",
    "\n",
    "\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "y_proba_rf = rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "print(\"===== Random Forest =====\")\n",
    "evaluate_model(y_test, y_pred_rf, y_proba_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame({\n",
    "    'feature': X_train_res.columns,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values(by='importance', ascending=True)  # ascending for horizontal bar\n",
    "\n",
    "# Plot horizontal bar chart\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(feature_importances['feature'], feature_importances['importance'], color='skyblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Random Forest Feature Importances')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "## XGboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_pos_weight = (y_train==0).sum() / (y_train==1).sum()  # original ratio\n",
    "scale_pos_weight_tuned = scale_pos_weight * 1.5  # increase weight for minority\n",
    "xgb = XGBClassifier(\n",
    "    learning_rate=0.01,\n",
    "    max_depth=3,\n",
    "    n_estimators=1000,\n",
    "    scale_pos_weight=scale_pos_weight_tuned,  # incresing weight for minority class\n",
    "    use_label_encoder=False,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb.fit(X_train_res, y_train_res)\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "y_proba_xgb = xgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"===== XGBoost =====\")\n",
    "evaluate_model(y_test, y_pred_xgb, y_proba_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "## Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_pos_weight = (y_train==0).sum() / (y_train==1).sum()  # original ratio\n",
    "scale_pos_weight_tuned = scale_pos_weight * 1.5  # increase weight for minority\n",
    "categorical_cols = ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']\n",
    "\n",
    "# Prepare Pool object for CatBoost (categorical features are indexed)\n",
    "cat_features_indices = [X_train_res.columns.get_loc(c) for c in categorical_cols]\n",
    "\n",
    "catboost_model = CatBoostClassifier(\n",
    "    iterations=500,\n",
    "    learning_rate=0.05,\n",
    "    depth=6,\n",
    "    eval_metric='Recall',\n",
    "    scale_pos_weight=scale_pos_weight_tuned,# incresing weight for minority class\n",
    "    random_state=42,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "catboost_model.fit(\n",
    "    X_train_res, y_train_res,\n",
    "    cat_features=cat_features_indices\n",
    ")\n",
    "\n",
    "y_pred = catboost_model.predict(X_test)\n",
    "y_proba = catboost_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "\n",
    "print(\"----Catboost-------\")\n",
    "evaluate_model(y_test, y_pred, y_proba)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "> ðŸŒŸ We observed that after removing those outliers(avg_glucose_level <=70 but stroke = 1) , we can we that our recall increase significantly "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "> ðŸ’¡ In conclusion, for healthcare applications, recall is prioritized over precision because false negatives are far more dangerous than false positives. Accepting lower precision is justified in order to ensure that critical cases are not missed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "## Explainable Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_maps = {\n",
    "    'gender': {0: 'Female', 1: 'Male', 2: 'Other'},\n",
    "    'ever_married': {0: 'No', 1: 'Yes'},\n",
    "    'work_type': {0: 'Govt_job', 1: 'Never_worked', 2: 'Private', 3: 'Self-employed', 4: 'children'},\n",
    "    'Residence_type': {0: 'Rural', 1: 'Urban'},\n",
    "    'smoking_status': {0: 'Unknown', 1: 'formerly smoked', 2: 'never smoked', 3: 'smokes'}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Map the catetory to human-readable\n",
    "def transformCategory(df):\n",
    "    df_copy = df.copy()  # avoid modifying original\n",
    "    for col, mapping in categorical_maps.items():\n",
    "        if col in df_copy.columns:  \n",
    "            df_copy[col] = df_copy[col].map(mapping)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install dice-ml -q\n",
    "import dice_ml\n",
    "\n",
    "d = dice_ml.Data(dataframe=X_train_res.assign(target=y_train_res),\n",
    "                  continuous_features=['age', 'avg_glucose_level', \"bmi\"],  \n",
    "                  outcome_name='target')  \n",
    "\n",
    "m = dice_ml.Model(model=rf, backend='sklearn')  \n",
    "exp = dice_ml.Dice(d, m, method='random')  \n",
    "\n",
    "# 4. Generate counterfactuals\n",
    "query_instance = X_test.iloc[4:5]  # can change this \n",
    "cf = exp.generate_counterfactuals(query_instance, total_CFs=3, desired_class=\"opposite\")\n",
    "cf_df = cf.cf_examples_list[0].final_cfs_df  \n",
    "\n",
    "transformCategory(cf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_target = rf.predict(query_instance)  \n",
    "query_instance.loc[:, \"target\"] = predicted_target\n",
    "transformCategory(query_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformCategory(cf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "# Counterfactual Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "## Generating Counter Factuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "### 1/\n",
    "\n",
    "We analyzed counterfactuals for a patient profile:\n",
    "\n",
    "| Gender | Age | Hypertension | Heart Disease | Married | Work Type | Residence | Avg Glucose | BMI  | Smoking Status | Target |\n",
    "|--------|-----|--------------|---------------|---------|-----------|-----------|-------------|------|----------------|--------|\n",
    "| Female | 43  | No           | No            | Yes     | Private   | Urban     | 86.67       | 33.3 | never smoked   | 0      |\n",
    "\n",
    "The model-generated counterfactuals for this patient are:\n",
    "\n",
    "| Gender | Age | Hypertension | Heart Disease | Married | Work Type | Residence | Avg Glucose | BMI  | Smoking Status | Target |\n",
    "|--------|-----|--------------|---------------|---------|-----------|-----------|-------------|------|----------------|--------|\n",
    "| Female | 43  | 0            | 0             | Yes     | Private   | Urban     | 110.87      | 33.3 | formerly smoked | 1      |\n",
    "| Female | 43  | 0            | 0             | Yes     | Private   | Urban     | 69.48       | 33.3 | never smoked   | 1      |\n",
    "| Female | 43  | 0            | 0             | Yes     | Private   | Urban     | 60.55       | 33.3 | never smoked   | 1      |\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "1. **Target change:** All counterfactuals flip the predicted stroke from 0 â†’ 1.  \n",
    "2. **Feature changes:** Only the **average glucose level** and **smoking status** vary significantly.  \n",
    "3. **Unrealistic pattern:**  \n",
    "   - Two counterfactuals suggest that **lowering average glucose level (69.48, 60.55)** increases stroke risk.  \n",
    "   - This is **not clinically plausible**, as lower glucose does not increase stroke probability.  \n",
    "\n",
    "### Actionable insight\n",
    "\n",
    "- This highlights that the model may have learned **spurious relationships** from rare cases.  \n",
    "- To generate **meaningful and medically realistic counterfactuals**, extreme or low-probability feature values should be filtered out during analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['stroke'][(data[\"avg_glucose_level\"]<=70) & (data['stroke'] == 1)].count() / data['stroke'][(data[\"avg_glucose_level\"]<=70) & (data['stroke'] == 0)].count() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "> Go deeper we can see that the case that the patient has **avg_glucose_level <= 70** and has stroke is only **3,7%**. So for more reasonable analysis, we should consider those as outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "### 2/\n",
    "\n",
    "We examined counterfactuals for the following patient profile:\n",
    "\n",
    "| Gender | Age | Hypertension | Heart Disease | Married | Work Type | Residence | Avg Glucose | BMI  | Smoking Status | Target |\n",
    "|--------|-----|--------------|---------------|---------|-----------|-----------|-------------|------|----------------|--------|\n",
    "| Male   | 17  | No           | No            | No      | Govt_job  | Urban     | 68.91       | 23.0 | Unknown        | 0      |\n",
    "\n",
    "After removing extreme or unrealistic outliers (e.g., extremely low or high glucose levels), the model-generated counterfactuals are:\n",
    "\n",
    "| Gender | Age  | Hypertension | Heart Disease | Married | Work Type | Residence | Avg Glucose | BMI  | Smoking Status | Target |\n",
    "|--------|------|--------------|---------------|---------|-----------|-----------|-------------|------|----------------|--------|\n",
    "| Male   | 76.6 | 0            | 0             | No      | Govt_job  | Urban     | 222.16      | 23.0 | Unknown        | 1      |\n",
    "| Male   | 57.1 | 0            | 0             | No      | Govt_job  | Urban     | 179.49      | 23.0 | Unknown        | 1      |\n",
    "| Male   | 65.8 | 0            | 0             | No      | Govt_job  | Urban     | 227.21      | 23.0 | Unknown        | 1      |\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "1. **Target change:** All counterfactuals flip the predicted stroke from 0 â†’ 1.  \n",
    "2. **Feature changes:** The primary changes are in **age** and **average glucose level**, which increase in the CFs.  \n",
    "3. **Reasonable pattern:**  \n",
    "   - Older age and higher glucose levels are associated with increased stroke risk.  \n",
    "   - This aligns with clinical expectations and literature.  \n",
    "\n",
    "### Actionable insight\n",
    "\n",
    "- By removing outliers from the dataset, the counterfactuals now reflect **plausible and medically meaningful scenarios**.  \n",
    "- This demonstrates that careful preprocessing (outlier removal) is essential for **trustworthy counterfactual explanations** in healthcare models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "### 3/\n",
    "\n",
    "Original patient profile:\n",
    "\n",
    "| Gender | Age | Hypertension | Heart Disease | Married | Work Type | Residence | Avg Glucose | BMI  | Smoking Status | Target |\n",
    "|--------|-----|--------------|---------------|---------|-----------|-----------|-------------|------|----------------|--------|\n",
    "| Female | 5   | No           | No            | No      | children  | Rural     | 102.04      | 18.5 | Unknown        | 0      |\n",
    "\n",
    "Counterfactuals generated:\n",
    "\n",
    "| Gender | Age  | Hypertension | Heart Disease | Married | Work Type   | Residence | Avg Glucose | BMI  | Smoking Status | Target |\n",
    "|--------|------|--------------|---------------|---------|------------|-----------|-------------|------|----------------|--------|\n",
    "| Female | 49.9 | 0            | 0             | No      | Private    | Rural     | 102.04      | 18.5 | Unknown        | 1      |\n",
    "| Female | 74.4 | 0            | 0             | No      | Never_worked | Rural   | 102.04      | 18.5 | Unknown        | 1      |\n",
    "| Female | 76.3 | 0            | 0             | No      | Govt_job   | Rural     | 102.04      | 18.5 | Unknown        | 1      |\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "1. **Target change:** All counterfactuals flip stroke prediction from 0 â†’ 1.  \n",
    "2. **Feature changes:** The main changes are **age** and **work type**, while glucose and BMI remain constant.  \n",
    "3. **Insight:**  \n",
    "   - Even at low BMI and normal glucose levels, older age and occupational stress factors (work type) increase predicted stroke risk.  \n",
    "   - This suggests that **stress-related factors might be important contributors** to stroke risk in the model.  \n",
    "\n",
    "### Actionable insight\n",
    "\n",
    "- Counterfactual analysis highlights that **non-traditional risk factors** such as age-related stress and occupational exposure can strongly influence predicted stroke risk.  \n",
    "- Proper handling of stress and lifestyle factors is crucial, especially for individuals in high-risk age groups.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "### 4/\n",
    "\n",
    "**Original patient profile:**  \n",
    "\n",
    "| Gender | Age  | Hypertension | Heart Disease | Married | Work Type | Residence | Avg Glucose | BMI  | Smoking Status | Target |\n",
    "|--------|------|--------------|---------------|---------|-----------|-----------|-------------|------|----------------|--------|\n",
    "| Male   | 61.0 | 1            | 1             | Yes     | Govt_job  | Rural     | 86.06       | 34.8 | never smoked   | 1      |\n",
    "\n",
    "**Counterfactuals generated:**  \n",
    "\n",
    "| Gender | Age  | Hypertension | Heart Disease | Married | Work Type | Residence | Avg Glucose | BMI   | Smoking Status | Target |\n",
    "|--------|------|--------------|---------------|---------|-----------|-----------|-------------|-------|----------------|--------|\n",
    "| Male   | 61.0 | 1            | 1             | Yes     | children  | Rural     | 86.06       | 96.1  | never smoked   | 0      |\n",
    "| Male   | 32.9 | 1            | 1             | Yes     | Govt_job  | Rural     | 86.06       | 34.8  | never smoked   | 0      |\n",
    "| Male   | 61.0 | 1            | 1             | Yes     | Govt_job  | Rural     | 67.00       | 34.8  | never smoked   | 0      |\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "1. **Target change:** All counterfactuals flip stroke prediction from 1 â†’ 0.  \n",
    "2. **Feature changes and plausibility:**  \n",
    "   - The most notable change is **BMI = 96.1** in one counterfactual, which is **clinically implausible**.  \n",
    "3. **Insight:**  \n",
    "   - The model appears **overly sensitive to extreme BMI increases**, which can artificially reduce predicted stroke risk.  \n",
    "   \n",
    "### Actionable insight\n",
    "\n",
    "- Counterfactual analysis reveals that **extreme BMI changes dominate the modelâ€™s predictions**, but these are **not realistic**.  \n",
    "- For meaningful interpretation, **filtering out implausible counterfactuals** is necessary.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[(data['bmi']>=50) & (data['stroke']==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[(data['bmi']>=50) & (data['stroke']==0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "#### Upon examining the dataset and generated counterfactuals, we observed several instances with **extremely high BMI values (â‰¥ 50)** that produce unrealistic or implausible predictions. Due to the dataset's high class imbalance, although higher BMI should generally increase stroke risk, the majority of these high-BMI instances have **stroke = 0**, which skews the modelâ€™s behavior. These instances can be considered **outliers**, and since they represent a small portion of the data, it is reasonable to remove them to improve the plausibility and reliability of subsequent analyses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "# LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install lime -q\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "# Setting up LIME\n",
    "feature_names = X_train_res.columns.tolist()\n",
    "class_names = ['No Stroke', 'Stroke']\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "    X_train_res.values,\n",
    "    feature_names=feature_names,\n",
    "    class_names=class_names,\n",
    "    discretize_continuous=True\n",
    ")\n",
    "\n",
    "# We'll analyze the same instances as the rest of the notebook\n",
    "instance_50 = X_test.iloc[50].values\n",
    "instance_100 = X_test.iloc[100].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "## Implementing LIME on RF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "> âš ï¸ Graphs key: Red pushes to No Stroke and Green to Stroke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instance 50\n",
    "print(\"Instance 50\")\n",
    "print(\"Predicted:\", class_names[rf.predict(pd.DataFrame(instance_50.reshape(1, -1), columns=X_train_res.columns))[0]])\n",
    "print(\"Actual:\", class_names[y_test.iloc[50]])\n",
    "exp_rf_50 = explainer.explain_instance(instance_50, rf.predict_proba, num_features=len(feature_names))\n",
    "exp_rf_50.as_pyplot_figure()\n",
    "plt.show()\n",
    "\n",
    "# Instance 100\n",
    "print(\"Instance 100\")\n",
    "print(\"Predicted:\", class_names[rf.predict(pd.DataFrame(instance_100.reshape(1, -1), columns=X_train_res.columns))[0]])\n",
    "print(\"Actual:\", class_names[y_test.iloc[100]])\n",
    "exp_rf_100 = explainer.explain_instance(instance_100, rf.predict_proba, num_features=len(feature_names))\n",
    "exp_rf_100.as_pyplot_figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {},
   "source": [
    "> Results\n",
    "\n",
    "Instance 50 (prediction: No Stroke):\n",
    "  - Strong protective (red): avg_glucose_level â‰¤ ~81.5 and bmi â‰¤ ~25.4 reduce risk.\n",
    "  - Mild risk (green): age in ~41â€“59 and some work_type effect add small positive pushes.\n",
    "  - Reading: Protective signals dominate overall, so the model predicts No Stroke.\n",
    "\n",
    "Instance 100 (prediction: Stroke):\n",
    "  - Dominant risk (green): age in ~59.5â€“74.5, work_type threshold, and higher avg_glucose_level.\n",
    "  - Local counterâ€‘effect (red): a BMI threshold can appear protective here due to local interactions; this does not imply BMI is globally protective.\n",
    "  - Reading: Age and glucose are the main drivers flipping the prediction to Stroke; other features reinforce the decision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "## Implementing LIME of XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instance 50\n",
    "print(\"Instance 50\")\n",
    "print(\"Predicted:\", class_names[xgb.predict(pd.DataFrame(instance_50.reshape(1, -1), columns=X_train_res.columns))[0]])\n",
    "print(\"Actual:\", class_names[y_test.iloc[50]])\n",
    "exp_xgb_50 = explainer.explain_instance(instance_50, xgb.predict_proba, num_features=len(feature_names))\n",
    "exp_xgb_50.as_pyplot_figure()\n",
    "plt.show()\n",
    "\n",
    "# Instance 100\n",
    "print(\"Instance 100\")\n",
    "print(\"Predicted:\", class_names[xgb.predict(pd.DataFrame(instance_100.reshape(1, -1), columns=X_train_res.columns))[0]])\n",
    "print(\"Actual:\", class_names[y_test.iloc[100]])\n",
    "exp_xgb_100 = explainer.explain_instance(instance_100, xgb.predict_proba, num_features=len(feature_names))\n",
    "exp_xgb_100.as_pyplot_figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {},
   "source": [
    "> Results\n",
    "\n",
    "Instance 50 (predicted: No Stroke)\n",
    "\n",
    "- avg_glucose_level â‰¤ ~81.5 is strongly protective.\n",
    "- bmi â‰¤ ~25.4 also reduces risk.\n",
    "- age in ~41â€“59 adds some local risk.\n",
    "- work_type threshold contributes a smaller positive push.\n",
    "Takeaway: Low glucose and normal BMI dominate, outweighing moderate age/work-related risk, so the model stays with No Stroke.\n",
    "\n",
    "Instance 100 (predicted: Stroke)\n",
    "- age in ~59.5â€“74.5 is the largest driver toward Stroke.\n",
    "- work_type threshold contributes meaningful positive risk.\n",
    "- avg_glucose_level > ~170 adds further risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "> Post Analysis Note:\n",
    "\n",
    "bmi > ~31.8 shows a negative (red) contribution here; that can happen locally due to feature interactions and how LIME bins features. It doesnâ€™t mean high BMI is globally protective, more so just that, for this patientâ€™s mix of values, BMI is not the main driver of the Stroke prediction.\n",
    "\n",
    "Takeaway: Age and high glucose chiefly flip the prediction to Stroke; other features reinforce the direction.\n",
    "\n",
    "- Practical note:\n",
    "  - Prefer medically plausible patterns (older age, higher glucose increasing risk). If LIME highlights implausible drivers, revisit preprocessing (e.g., outlier handling) or constraints.\n",
    "\n",
    "- Reminder:\n",
    "  - LIME is local to the chosen instance and explains â€œwhy this prediction here,â€ not global importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "## Interactive Widget for all Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install ipywidgets -q\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# UI controls\n",
    "idx_slider = widgets.IntSlider(value=0, min=0, max=len(X_test)-1, step=1, description='Instance')\n",
    "model_picker = widgets.Dropdown(options=[('Random Forest', 'rf'), ('XGBoost', 'xgb')], value='rf', description='Model')\n",
    "output = widgets.Output()\n",
    "\n",
    "# Rendering with XGB and RF\n",
    "def render(_=None):\n",
    "    idx = idx_slider.value\n",
    "    model = rf if model_picker.value == 'rf' else xgb\n",
    "    x = X_test.iloc[idx].values\n",
    "\n",
    "    # Predicted/Actual labels\n",
    "    pred_label = model.predict(pd.DataFrame(x.reshape(1, -1), columns=X_train_res.columns))[0]\n",
    "    actual_label = y_test.iloc[idx]\n",
    "\n",
    "    with output:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Instance {idx}\")\n",
    "        print(\"Predicted:\", class_names[pred_label])\n",
    "        print(\"Actual:\", class_names[actual_label])\n",
    "\n",
    "        # Instance values displaying\n",
    "        try:\n",
    "            decoded = transformCategory(X_test.iloc[[idx]])\n",
    "            print(\"\\nInstance values:\")\n",
    "            display(decoded)\n",
    "        except Exception:\n",
    "            # Incase transformCategory isn't available\n",
    "            pass\n",
    "\n",
    "        # Integrating LIME\n",
    "        exp = explainer.explain_instance(\n",
    "            x,\n",
    "            lambda d: model.predict_proba(pd.DataFrame(d, columns=feature_names)),\n",
    "            num_features=len(feature_names)\n",
    "        )\n",
    "        exp.as_pyplot_figure()\n",
    "        plt.show()\n",
    "\n",
    "        # Analyzing each local condition and its influence on the prediction\n",
    "        print(\"\\nVaraibles and their influence:\")\n",
    "        for rule, weight in exp.as_list():\n",
    "            print(f\"- {rule} â†’ {weight:.3f}\")\n",
    "\n",
    "# Interactions\n",
    "idx_slider.observe(render, names='value')\n",
    "model_picker.observe(render, names='value')\n",
    "\n",
    "#UI\n",
    "controls = widgets.HBox([idx_slider, model_picker])\n",
    "display(controls)\n",
    "display(output)\n",
    "\n",
    "# Initial render\n",
    "render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {},
   "source": [
    "## Stats of all Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(X_test)\n",
    "\n",
    "rf_probs, xgb_probs = [], []\n",
    "rf_scores, xgb_scores = [], []\n",
    "\n",
    "for idx in range(N):\n",
    "    x = X_test.iloc[idx].values\n",
    "    df = pd.DataFrame(x.reshape(1, -1), columns=feature_names)\n",
    "    \n",
    "    # Original model probabilities\n",
    "    rf_probs.append(rf.predict_proba(df)[0, 1])\n",
    "    xgb_probs.append(xgb.predict_proba(df)[0, 1])\n",
    "    \n",
    "    # Comparing local surrogate fidelity with the model\n",
    "    exp_rf = explainer.explain_instance(\n",
    "        x, \n",
    "        lambda d: rf.predict_proba(pd.DataFrame(d, columns=feature_names)),\n",
    "        num_features=5  # fewer features for speed\n",
    "    )\n",
    "    rf_scores.append(getattr(exp_rf, 'score', np.nan))\n",
    "    \n",
    "    exp_xgb = explainer.explain_instance(\n",
    "        x, \n",
    "        lambda d: xgb.predict_proba(pd.DataFrame(d, columns=feature_names)),\n",
    "        num_features=10\n",
    "    )\n",
    "    xgb_scores.append(getattr(exp_xgb, 'score', np.nan))\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    'model': ['Random Forest', 'XGBoost'],\n",
    "    'proba_mean': [np.mean(rf_probs), np.mean(xgb_probs)],\n",
    "    'proba_median': [np.median(rf_probs), np.median(xgb_probs)],\n",
    "    'proba_std': [np.std(rf_probs), np.std(xgb_probs)],\n",
    "    'lime_fidelity_mean': [np.nanmean(rf_scores), np.nanmean(xgb_scores)],\n",
    "    'lime_fidelity_median': [np.nanmedian(rf_scores), np.nanmedian(xgb_scores)]\n",
    "})\n",
    "\n",
    "print(f\"Sample size: {N}\")\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "### Insights and other visualizations thus far\n",
    "\n",
    "- Stroke probability (original model outputs)\n",
    "  - XGBoost shows higher mean/median stroke probability than Random Forest. This essentailly tells us XGBoost is more riskâ€‘sensitive and will flag more patients as likely stroke. This is good! Recall over Precision is best in medical fields\n",
    "  - XGBoostâ€™s probabilities are also more spread out (higher variability), while Random Forest looks more conservative/stable. Expect XGBoost to boost recall but potentially lower precision.\n",
    "\n",
    "- LIME fidelity (how well the local surrogate mimics the model)\n",
    "  - Average fidelity is modest (roughly in the 0.27â€“0.33 range, while the median is lower). Normal for fast, lightweight settings and reminds us LIME is an approximation.\n",
    "  - Use LIME primarily for the direction and main drivers (which features push toward Stroke vs No Stroke). Avoid overâ€‘interpreting tiny weight differences.\n",
    "  - We are aware that fidelity can be improved by adjusting kernel width, or allowing more features in the local surrogate, but as far as analysis has to go, this suffices + kept it small for speed\n",
    "\n",
    "  Below we will look at two metrics that help us understand the LIME analysis in a more quantitative way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "## LIME Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "### LIME Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF: Average absolute LIME weight per feature for the top 10 features\n",
    "\n",
    "N = len(X_test)\n",
    "weights = pd.Series(0.0, index=feature_names)\n",
    "counts = pd.Series(0, index=feature_names)\n",
    "\n",
    "for idx in range(N):\n",
    "    x = X_test.iloc[idx].values\n",
    "    exp = explainer.explain_instance(\n",
    "        x,\n",
    "        lambda d: rf.predict_proba(pd.DataFrame(d, columns=feature_names)),\n",
    "        num_features=10\n",
    "    )\n",
    "    for rule, w in exp.as_list():\n",
    "        fname = rule.split(' ')[0]\n",
    "        if fname in weights.index:\n",
    "            weights[fname] += abs(w)\n",
    "            counts[fname] += 1\n",
    "\n",
    "avg_abs = (weights / counts.replace(0, np.nan)).dropna().sort_values(ascending=False).head(10)\n",
    "plt.figure(figsize=(8, 4))\n",
    "avg_abs.plot(kind='barh', color='teal')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('RF: Avg absolute LIME weight (first 300 instances)')\n",
    "plt.xlabel('|weight|')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGB: Average absolute LIME weight per feature for the top 10 features\n",
    "\n",
    "N = len(X_test)\n",
    "weights = pd.Series(0.0, index=feature_names)\n",
    "counts = pd.Series(0, index=feature_names)\n",
    "\n",
    "for idx in range(N):\n",
    "    x = X_test.iloc[idx].values\n",
    "    exp = explainer.explain_instance(\n",
    "        x,\n",
    "        lambda d: xgb.predict_proba(pd.DataFrame(d, columns=feature_names)),\n",
    "        num_features=10\n",
    "    )\n",
    "    for rule, w in exp.as_list():\n",
    "        fname = rule.split(' ')[0]\n",
    "        if fname in weights.index:\n",
    "            weights[fname] += abs(w)\n",
    "            counts[fname] += 1\n",
    "\n",
    "avg_abs = (weights / counts.replace(0, np.nan)).dropna().sort_values(ascending=False).head(10)\n",
    "plt.figure(figsize=(8, 4))\n",
    "avg_abs.plot(kind='barh', color='purple')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('XGB: Avg absolute LIME weight (first 300 instances)')\n",
    "plt.xlabel('|weight|')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {},
   "source": [
    "Average absolute LIME weights tells us how strongly a feature influences the model's prediction across many local LIME explanations. We take the absolute value of the weight bec in this case we are more interested in the influence, regardless of whether it is positive or negative. This allows us to then average values across many instances and understand which features are typically most impactful locally.\n",
    "\n",
    "> Insights\n",
    "\n",
    "In both models, the top two features with the most impact are the the same, namely, age and average glucose level. Comparing this with scientific literature, this checks out. These results go a long way into building trust on the model. \n",
    "\n",
    "However, at the same time, something suspicious is that prior hypertension and heart disease existence has almost no influence (<0.1) on the prediction. As a reminder, we remember that these magnitudes are local, and they depend on LIME binning. They should be used together with fidelity (see below)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92",
   "metadata": {},
   "source": [
    "### LIME Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIME fidelity distribution (RF vs XGB) with counts\n",
    "\n",
    "N = min(300, len(X_test))\n",
    "rf_scores, xgb_scores = [], []\n",
    "\n",
    "for idx in range(N):\n",
    "    x = X_test.iloc[idx].values\n",
    "    exp_rf = explainer.explain_instance(\n",
    "        x,\n",
    "        lambda d: rf.predict_proba(pd.DataFrame(d, columns=feature_names)),\n",
    "        num_features=10\n",
    "    )\n",
    "    rf_scores.append(getattr(exp_rf, 'score', np.nan))\n",
    "\n",
    "    exp_xgb = explainer.explain_instance(\n",
    "        x,\n",
    "        lambda d: xgb.predict_proba(pd.DataFrame(d, columns=feature_names)),\n",
    "        num_features=10\n",
    "    )\n",
    "    xgb_scores.append(getattr(exp_xgb, 'score', np.nan))\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "counts_list, bins, containers = plt.hist([rf_scores, xgb_scores], bins=20, label=['RF', 'XGB'], alpha=0.7)\n",
    "plt.title('LIME fidelity (local surrogate R^2) distribution')\n",
    "plt.xlabel('R^2 (score)')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "\n",
    "# Annotation of counts\n",
    "for counts, container in zip(counts_list, containers):\n",
    "    for c, rect in zip(counts, container.patches):\n",
    "        if c > 0:\n",
    "            x = rect.get_x() + rect.get_width() / 2\n",
    "            y = rect.get_height()\n",
    "            plt.text(x, y + max(0.5, y * 0.02), str(int(round(c))), ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94",
   "metadata": {},
   "source": [
    "This shows us the distribution of the local surrogate RÂ² (fidelity) for RF and XGB. For the right cluster i.e. R^2 of ~0.5-0.7 or so, both models perform fairly well. In this cluster, we can trust both the direction and the magnitude of the feature weights.\n",
    "\n",
    "For the left cluster, we have low fidelity scores. It tells us that the local linear surrogate struggles to explain the predictions. We can only use LIME for a rough directional hint here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probability vs LIME fidelity (RF and XGB)\n",
    "\n",
    "N = min(300, len(X_test))\n",
    "rf_probs, xgb_probs = [], []\n",
    "rf_scores, xgb_scores = [], []\n",
    "\n",
    "for idx in range(N):\n",
    "    x = X_test.iloc[idx].values\n",
    "    df = pd.DataFrame(x.reshape(1, -1), columns=feature_names)\n",
    "\n",
    "    # model probabilities\n",
    "    rf_probs.append(rf.predict_proba(df)[0, 1])\n",
    "    xgb_probs.append(xgb.predict_proba(df)[0, 1])\n",
    "\n",
    "    # LIME fidelity (local surrogate R^2)\n",
    "    exp_rf = explainer.explain_instance(\n",
    "        x,\n",
    "        lambda d: rf.predict_proba(pd.DataFrame(d, columns=feature_names)),\n",
    "        num_features=10\n",
    "    )\n",
    "    rf_scores.append(getattr(exp_rf, 'score', np.nan))\n",
    "\n",
    "    exp_xgb = explainer.explain_instance(\n",
    "        x,\n",
    "        lambda d: xgb.predict_proba(pd.DataFrame(d, columns=feature_names)),\n",
    "        num_features=10\n",
    "    )\n",
    "    xgb_scores.append(getattr(exp_xgb, 'score', np.nan))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n",
    "axes[0].scatter(rf_probs, rf_scores, alpha=0.6, color='skyblue', s=18)\n",
    "axes[0].set_title('RF: prob vs LIME fidelity')\n",
    "axes[0].set_xlabel('Predicted stroke probability')\n",
    "axes[0].set_ylabel('LIME fidelity (R^2)')\n",
    "\n",
    "axes[1].scatter(xgb_probs, xgb_scores, alpha=0.6, color='orange', s=18)\n",
    "axes[1].set_title('XGB: prob vs LIME fidelity')\n",
    "axes[1].set_xlabel('Predicted stroke probability')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96",
   "metadata": {},
   "source": [
    "> Predicted probability vs LIME fidelity â€” insights\n",
    "\n",
    "- RF:\n",
    "    - We see two clusterings, a higherâ€‘fidelity band around ~0.55â€“0.65 (mostly at lower risk) and a lower band around ~0.1â€“0.25 across the rest.\n",
    "    - This suggests RFâ€™s decisions are easier to approximate locally for many lowâ€‘risk predictions, while midâ€‘toâ€‘high risk cases often look more complex (lower RÂ²).\n",
    "- XGB:\n",
    "    - More spread and generally lower fidelity on average than RF, with many points clustered at low RÂ² (â‰ˆ0.05â€“0.2) across the full probability range.\n",
    "    - This hints XGBâ€™s decision boundary is more nonlinear/complex, making a simple local surrogate (like LIMEâ€™s linear model) fit less tightly.\n",
    "\n",
    "> Fidelity:\n",
    "\n",
    "Fidelity is a trust measure for local explanations. It tells us how well the LIME surrogate model represents the original model's outputs on the samples. A high fidelity i.e. close to 1 tells us that the surrogate matches the original model fairly well and vice versa. \n",
    "\n",
    "With a high fidelity i.e. >0.6, we can trust both the direction and the relative magnitude of the LIME weights. A moderate fidelity i.e. 0.3-0.6, we should treat LIME as a mere directional insight i.e. what features push the predictions up/down. As for low fidelity i.e. <0.3 we should realize that the local linear surrogate does not do a very good job of fitting to the model. \n",
    "\n",
    "In this case, in the cluster of High RÂ², we can treat the LIME weights as a more faithful picture of the modelâ€™s local behavior. Meanwhile for Low RÂ², we use LIME weights for rough influence measure i.e. which features push up or down risk. Again, this is not a precise magnitude, thus results should be cross checked with a professional for instance.\n",
    "\n",
    "> Conclusions:\n",
    "\n",
    "We can see that on average that our LIME implementation does a better job at approximating to the RF compared to XGB. \n",
    "What next? this can be studied and compared with other datasets to see whether LIME works better with RF as compared to XGB. If this is indeed the case then this will help us contribute towards the open field of xAI research.\n",
    "Moreover, after this visualization, we can tell the doctors which model to trust more depending on whether they want to prioritize recall or precision in their predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97",
   "metadata": {},
   "source": [
    "## SP-LIME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {},
   "source": [
    "Up until now, our analysis has been more local and not necessarily representative of the overall model. In the following section, with a motivation to garner a more global analysis, we will implement SP-LIME, \"a method that selects a set of representative instances with explanations to address the â€œtrusting the model problem, via submodular optimization.\" [1]\n",
    "\n",
    "> [1] Quote from Ribeiro, M. T., Singh, S., & Guestrin, C. (2016). \"Why Should I Trust You?\": Explaining the Predictions of Any Classifier. ArXiv. https://arxiv.org/abs/1602.04938"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SP-LIME Implementation, submodular pick for global and representative explanations for RF & XGB\n",
    "N = len(X_test)\n",
    "D = X_test.iloc[:N].values\n",
    "\n",
    "# RF\n",
    "sp_rf = SubmodularPick(\n",
    "    explainer,\n",
    "    D,\n",
    "    lambda d: rf.predict_proba(pd.DataFrame(d, columns=feature_names)),\n",
    "    num_features=10,\n",
    "    num_exps_desired=8\n",
    ")\n",
    "\n",
    "# XGB\n",
    "sp_xgb = SubmodularPick(\n",
    "    explainer,\n",
    "    D,\n",
    "    lambda d: xgb.predict_proba(pd.DataFrame(d, columns=feature_names)),\n",
    "    num_features=10,\n",
    "    num_exps_desired=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the SP-LIME representative instances\n",
    "\n",
    "rf_positions = list(getattr(sp_rf, 'V', []))\n",
    "xgb_positions = list(getattr(sp_xgb, 'V', []))\n",
    "\n",
    "print(\"SP-LIME RF representative instances:\")\n",
    "print(f\"Positions in X_test: {rf_positions}\")\n",
    "display(X_test.iloc[rf_positions].reset_index(drop=True))\n",
    "\n",
    "print(\"SP-LIME XGB representative instances:\")\n",
    "print(f\"Positions in X_test: {xgb_positions}\")\n",
    "display(X_test.iloc[xgb_positions].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101",
   "metadata": {},
   "source": [
    "> Key: \n",
    "\n",
    "'gender': {0: 'Female', 1: 'Male'},\n",
    "\n",
    "'ever_married': {0: 'No', 1: 'Yes'},\n",
    "\n",
    "'work_type': {0: 'Govt_job', 1: 'Never_worked', 2: 'Private', 3: 'Self-employed', 4: 'children'},\n",
    "\n",
    "'Residence_type': {0: 'Rural', 1: 'Urban'},\n",
    "\n",
    "'smoking_status': {0: 'Unknown', 1: 'formerly smoked', 2: 'never smoked', 3: 'smokes'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102",
   "metadata": {},
   "source": [
    "> Insights\n",
    "\n",
    "Submodular Pick (SP-LIME) selected a small, representative set of instances from `X_test` that best covers the diversity of local LIME explanations. In our case, we have generated 8 such instances, displayed above. The motivation stems from the fact that instance-by-instance LIME, what we did in the last section, is local. Here, SP-LIME lifts it to a global view without scanning the entire dataset.\n",
    "\n",
    "> Conclusions:\n",
    "\n",
    "- Across both RF and XGB, age and avg_glucose_level consistently appear as top drivers, with bmi and work_type also influential. \n",
    "- Cardiovascular indicators (hypertension, heart_disease) contribute but with smaller average weights than age/glucose. \n",
    "- The representative subset reflects diverse categorical combinations (gender, residence, marriage), which helps capture different decision regions. Decoding confirms categories are sensible and interpretable (no unexpected codes). This improves the trust and use of instance level reviews for clinicians and stakeholders.\n",
    "\n",
    "\n",
    "> What the results show:\n",
    "\n",
    "- Consistent global drivers across models:\n",
    "  - Age is the strongest contributor in both RF and XGB.\n",
    "  - Avg glucose level and BMI are also influential, with work_type showing moderate importance.\n",
    "- Additional, smaller contributors:\n",
    "  - Residence_type and smoking_status have non-trivial but smaller average impacts.\n",
    "  - Binary comorbids (hypertension, heart_disease) contribute less on average globally, but can be decisive for specific individuals.\n",
    "- Local variability:\n",
    "  - Across the representative set, feature importance patterns vary by instance, reinforcing that LIME explanations are local.\n",
    "  - Previously observed LIME fidelity scores (exp.score) and distributions indicate decent but instance-dependent surrogate fit.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

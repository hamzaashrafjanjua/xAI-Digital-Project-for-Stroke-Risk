{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Digital Project for Data Science and Business Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    " > The primary objective of this project is to analyze how black-box machine learning models make predictions and determine what factors impact these predictions the most. Although it is important for models to make good predictions, this project is especially concerned with interpretability because it is important that the reasoning behind predictions is understood, especially in areas such as medicine, where decisions have serious consequences.\n",
    "\n",
    "For this, various Explainable Artificial Intelligence (xAI) methods are used on stroke risk prediction models developed on the Stroke dataset, which is publicly available on Kaggle. The aim here is not only to produce explanations but also to critically examine the validity, reliability, and relevance of those explanations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Imports and Initial Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report, precision_recall_curve\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from catboost import CatBoostClassifier\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "from lime.submodular_pick import SubmodularPick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./healthcare-dataset-stroke-data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data.select_dtypes(include=\"object\").columns :\n",
    "    data[col] = data[col].astype(\"category\")\n",
    "data = data.drop(columns=[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "| Feature             | Type               | Description                           | Possible Values / Format                                           | Explanation / Notes                                               |\n",
    "| ------------------- | ------------------ | ------------------------------------- | ------------------------------------------------------------------ | -----------------------------------\n",
    "| `gender`            | Categorical        | Biological sex of the patient         | `Male`, `Female`                                                   | Can be used as a feature; may need encoding for ML models         |\n",
    "| `age`               | Numerical          | Age of the patient in years           | Float                                                              | Important risk factor for stroke; higher age often increases risk |\n",
    "| `hypertension`      | Categorical/Binary | Whether patient has hypertension      | `0` (No), `1` (Yes)                                                | Hypertension is a key stroke risk factor                          |\n",
    "| `heart_disease`     | Categorical/Binary | Whether patient has any heart disease | `0` (No), `1` (Yes)                                                | Another major risk factor for stroke                              |\n",
    "| `ever_married`      | Categorical        | Marital status                        | `Yes`, `No`                                                        | Could correlate with lifestyle or social support                  |\n",
    "| `work_type`         | Categorical        | Type of occupation                    | `Private`, `Self-employed`, `Govt_job`, `Children`, `Never_worked` | Can reflect lifestyle and stress levels                           |\n",
    "| `Residence_type`    | Categorical        | Urban or rural living area            | `Urban`, `Rural`                                                   | Can affect access to healthcare and lifestyle patterns            |\n",
    "| `avg_glucose_level` | Numerical          | Average blood glucose level (mg/dL)   | Float                                                              | High glucose/diabetes increases stroke risk                       |\n",
    "| `bmi`               | Numerical          | Body Mass Index                       | Float (may contain NaN)                                            | Obesity is a risk factor; missing values may need imputation      |\n",
    "| `smoking_status`    | Categorical        | Smoking habits                        | `never smoked`, `formerly smoked`, `smokes`, `Unknown`             | Smoking is a major risk factor; `Unknown` needs special handling  |\n",
    "| `stroke`            | Categorical/Binary | Whether patient has had a stroke      | `0` (No), `1` (Yes)                                                | Target variable for prediction                                    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Unvalidated values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "> The minimum value of the age column is 0.008 years , which is suspicious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Distribution of Age\")\n",
    "plt.ylabel(\"Age\")\n",
    "plt.hist(data[\"age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data[\"age\"]<=1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "> ðŸ’¡ Upon examining the dataset, the small age values (e.g., 0.08, 0.16, 0.32 years) correspond to children under 1 year old.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# Data Cleaning & Feature Engineering + Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## NaN Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Data distribution before filling Nan\")\n",
    "plt.xlabel(\"BMI\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.hist(data[\"bmi\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Fill NaN with median since the data distribution is skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"bmi\"] = data[\"bmi\"].fillna(data[\"bmi\"].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Data distribution after filling Nan\")\n",
    "plt.xlabel(\"BMI\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.hist(data[\"bmi\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Duplicated Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of duplicated rows: \", data.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## Detecting Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data.drop(columns=[\"stroke\"]).select_dtypes(\"number\").columns:\n",
    "    plt.figure()  \n",
    "    plt.title(f\"Data distribution of {col}\")\n",
    "    plt.hist(data[col])  \n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = data.drop(columns=[\"hypertension\", \"heart_disease\", \"stroke\"]).select_dtypes(include=\"number\").columns\n",
    "\n",
    "for col in numeric_cols:\n",
    "    plt.figure(figsize=(6, 4))  \n",
    "    plt.title(f\"Box plot of {col}\")\n",
    "    plt.boxplot(data[col])\n",
    "    plt.xlabel(col)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "> ðŸ’¡ Although there are some outliers in bmi and avg_glucose_level, We will primarily use tree-based models (e.g., Random Forest, XGBoost, CatBoost), which are naturally robust to outliers. Therefore, these extreme values will be retained, as they may carry important information for predicting stroke."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Data Cleaning Based on Counterfactual Analysis\n",
    "\n",
    "Upon examining the dataset and generated counterfactuals, we identified **two types of clinically implausible cases**:\n",
    "\n",
    "1. **Extremely low glucose levels** â€“ Some counterfactuals suggested that lowering glucose would increase stroke risk. This is **not supported by medical evidence** and occurs only in a small subset of patients (rare outliers).  \n",
    "\n",
    "2. **Extremely high BMI values (â‰¥ 50)** â€“ A few instances indicated that very high BMI decreases stroke risk, which is **clinically unrealistic** and also represents a small portion of the dataset.  \n",
    "\n",
    "Given that these cases are rare and potentially misleading, it is reasonable to remove them to **improve the plausibility and reliability** of subsequent analyses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where avg_glucose_level <= 70 AND stroke == 1\n",
    "data = data[~((data['avg_glucose_level'] <= 70) & (data['stroke'] == 1))]\n",
    "data = data[~(data['bmi'] >= 50)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "## Imbalanced target classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Data distribution of stroke\")\n",
    "plt.hist(data[\"stroke\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### So the target value is highly imbalanced. I will using SMOTE to oversampling the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select_dtypes(\"number\").corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "# Building models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_mappings = {}\n",
    "dataForModel = data.copy()\n",
    "for col in ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']:\n",
    "    dataForModel[col] = dataForModel[col].astype('category')\n",
    "    category_mappings[col] = dict(enumerate(dataForModel[col].cat.categories))\n",
    "    dataForModel[col] = dataForModel[col].cat.codes\n",
    "\n",
    "X = dataForModel.drop(columns=\"stroke\")\n",
    "y = dataForModel[\"stroke\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "plt.hist(y_train_res)\n",
    "plt.title(\"Target classes after using SMOTE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "### Function to calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(y_true, y_pred, y_proba):\n",
    "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(\"Precision:\", precision_score(y_true, y_pred))\n",
    "    print(\"Recall:\", recall_score(y_true, y_pred))\n",
    "    print(\"F1-Score:\", f1_score(y_true, y_pred))\n",
    "    print(\"ROC-AUC:\", roc_auc_score(y_true, y_proba))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(cm)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sn.heatmap(\n",
    "        cm, \n",
    "        annot=True, \n",
    "        fmt=\"d\", \n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=[\"Predicted No Stroke\", \"Predicted Stroke\"],\n",
    "        yticklabels=[\"Actual No Stroke\", \"Actual Stroke\"]\n",
    "    )\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "## Random forest "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "#### Tunning hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 500],      # number of trees\n",
    "    'max_depth': [None, 5, 10, 20],      # max depth of tree\n",
    "    'min_samples_split': [2, 5, 10],     # min samples to split\n",
    "    'min_samples_leaf': [1, 2, 4],       # min samples per leaf\n",
    "    'class_weight': ['balanced', {0:1,1:5}, {0:1,1:10}]  # emphasize minority\n",
    "}\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    scoring='recall',  # maximize recall for minority class\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "\n",
    "grid_search.fit(X_train_res, y_train_res)\n",
    "\n",
    "print(\"Best parameters found:\", grid_search.best_params_)\n",
    "print(\"Best recall score:\", grid_search.best_score_)\n",
    "\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "y_proba_rf = best_rf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(\"===== Random Forest (Grid Search) =====\")\n",
    "evaluate_model(y_test, y_pred_rf, y_proba_rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "#### Using tunned hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=5,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=1,\n",
    "    class_weight={0: 1, 1: 2},  \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "rf.fit(X_train_res, y_train_res)\n",
    "\n",
    "\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "y_proba_rf = rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "print(\"===== Random Forest =====\")\n",
    "evaluate_model(y_test, y_pred_rf, y_proba_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame({\n",
    "    'feature': X_train_res.columns,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values(by='importance', ascending=True)  # ascending for horizontal bar\n",
    "\n",
    "# Plot horizontal bar chart\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(feature_importances['feature'], feature_importances['importance'], color='skyblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Random Forest Feature Importances')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "## XGboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_pos_weight = (y_train==0).sum() / (y_train==1).sum()  # original ratio\n",
    "scale_pos_weight_tuned = scale_pos_weight * 1.5  # increase weight for minority\n",
    "xgb = XGBClassifier(\n",
    "    learning_rate=0.01,\n",
    "    max_depth=3,\n",
    "    n_estimators=1000,\n",
    "    scale_pos_weight=scale_pos_weight_tuned,  # incresing weight for minority class\n",
    "    use_label_encoder=False,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb.fit(X_train_res, y_train_res)\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "y_proba_xgb = xgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"===== XGBoost =====\")\n",
    "evaluate_model(y_test, y_pred_xgb, y_proba_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "## Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_pos_weight = (y_train==0).sum() / (y_train==1).sum()  # original ratio\n",
    "scale_pos_weight_tuned = scale_pos_weight * 1.5  # increase weight for minority\n",
    "categorical_cols = ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']\n",
    "\n",
    "# Prepare Pool object for CatBoost (categorical features are indexed)\n",
    "cat_features_indices = [X_train_res.columns.get_loc(c) for c in categorical_cols]\n",
    "\n",
    "catboost_model = CatBoostClassifier(\n",
    "    iterations=500,\n",
    "    learning_rate=0.05,\n",
    "    depth=6,\n",
    "    eval_metric='Recall',\n",
    "    scale_pos_weight=scale_pos_weight_tuned,# incresing weight for minority class\n",
    "    random_state=42,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "catboost_model.fit(\n",
    "    X_train_res, y_train_res,\n",
    "    cat_features=cat_features_indices\n",
    ")\n",
    "\n",
    "y_pred = catboost_model.predict(X_test)\n",
    "y_proba = catboost_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "\n",
    "print(\"----Catboost-------\")\n",
    "evaluate_model(y_test, y_pred, y_proba)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "> ðŸŒŸ We observed that after removing those outliers(avg_glucose_level <=70 but stroke = 1) , we can we that our recall increase significantly "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "> ðŸ’¡ In conclusion, for healthcare applications, recall is prioritized over precision because false negatives are far more dangerous than false positives. Accepting lower precision is justified in order to ensure that critical cases are not missed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "# xAI Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "# SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "While model performance metrics indicate how well a classifier performs, they do not explain why predictions are made. To address this limitation, we use SHAP (SHapley Additive exPlanations) to interpret the behavior of our stroke prediction model at both global and local levels. Unlike LIME, SHAP provides explanations that are theoretically grounded and consistent across different predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install shap -q\n",
    "\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SHAP explainer for catboost and xgboost models\n",
    "explainer_xgb = shap.Explainer(xgb)\n",
    "\n",
    "explainer_cat = shap.Explainer(catboost_model)\n",
    "\n",
    "# Compute SHAP values for the test set\n",
    "shap_values_xgb = explainer_xgb(X_test)\n",
    "\n",
    "shap_values_cat = explainer_cat(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "The SHAP explainer computes contribution values for each feature by comparing model predictions against a baseline (expected value). Each SHAP value represents how much a feature increases or decreases the prediction for a given instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "### Global Explanation: Feature Importance\n",
    "\n",
    "The purpose of global explaination is to understand which features most influence stroke predictions overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values_xgb, X_test, show=False)\n",
    "plt.title(\"SHAP Summary plot for XGBoost stroke prediction model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values_cat, X_test, show=False)\n",
    "plt.title(\"SHAP Summary plot for CatBoost stroke prediction model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "The SHAP summary plots ranks features according to their average absolute contribution to the model output. Features at the top of the plot, such as age and average glucose level, have the greatest influence on predictions. The color gradient indicates whether high or low feature values increase or decrease predicted stroke risk.\n",
    "\n",
    "ðŸ“Œ Key Insight\n",
    "\n",
    "- Higher age and glucose levels consistently increase predicted risk\n",
    "- Individuals with midâ€‘range BMI values appear to have a higher risk of stroke than those at either very high or very low BMI levels\n",
    "- Several categorical variables show weaker or context-dependent influence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "### Local Explanation: Individual Prediction\n",
    "\n",
    "The purpose of local explaination is to know why a specific patient was predicted as high or low risk. In simple words, local explaination tells us which features played an important role (for model) for a specific patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 49\n",
    "idx_2 = 99\n",
    "\n",
    "shap.plots.waterfall(shap_values_xgb[idx], show=False)\n",
    "plt.title(\"SHAP Waterfall plot of patient 50 for XGBoost stroke prediction model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.waterfall(shap_values_xgb[idx_2], show=False)\n",
    "plt.title(\"SHAP Waterfall plot of patient 100 for XGBoost stroke prediction model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.waterfall(shap_values_cat[idx], show=False)\n",
    "plt.title(\"SHAP Waterfall plot of patient 50 for CatBoost stroke prediction model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.waterfall(shap_values_cat[idx_2], show=False)\n",
    "plt.title(\"SHAP Waterfall plot of patient 100 for CatBoost stroke prediction model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "The waterfall plots illustrates how individual feature contributions combine to produce the final prediction. Features pushing the prediction toward higher stroke risk are shown in red, while protective features are shown in blue. This allows for intuitive, patient-level interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "### Feature Effect Analysis (Dependence Plots)\n",
    "\n",
    "This is to analyze how feature values relate to their SHAP contributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "#### Influence of Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap_values_xgb[:, \"age\"], show=False)\n",
    "plt.title(\"SHAP Scatter plot showing the influence of Age on stroke risk predictions in the XGBoost model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap_values_cat[:, \"age\"], show=False)\n",
    "plt.title(\"SHAP Scatter plot showing the influence of Age on stroke risk predictions in the CatBoost model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "The SHAP scatter plots shows how the SHAP value of age changes across its range. Older patients exhibit consistently higher SHAP values, confirming that age is a dominant driver of stroke risk in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "#### Influence of Average Glucose Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap_values_xgb[:, \"avg_glucose_level\"], show=False)\n",
    "plt.title(\"SHAP Scatter plot showing the influence of Average Glucose Level on stroke risk predictions in the XGBoost model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap_values_cat[:, \"avg_glucose_level\"], show=False)\n",
    "plt.title(\"SHAP Scatter plot showing the influence of Average Glucose Level on stroke risk predictions in the CatBoost model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "The SHAP scatter plots indicate that lower average glucose levels contribute negatively to stroke risk predictions, while higher glucose values increasingly push the model toward a higher predicted risk. This pattern is consistent across both the CatBoost and XGBoost models, suggesting that average glucose level is a strong and stable risk factor influencing stroke predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "#### Influence of Age with Hypertension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap_values_xgb[:, \"age\"], color=shap_values_xgb[:, \"hypertension\"], show=False)\n",
    "plt.title(\"SHAP Scatter plot showing the influence of Age with Hypertension on stroke risk predictions in the XGBoost model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap_values_cat[:, \"age\"], color=shap_values_cat[:, \"hypertension\"], show=False)\n",
    "plt.title(\"SHAP Scatter plot showing the influence of Age with Hypertension on stroke risk predictions in the CatBoost model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "This visualization reveals interactions between age and hypertension, indicating that the effect of age on stroke risk is amplified for patients with hypertension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "#### Influence of Average Glucose Level with Hypertension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap_values_xgb[:, \"avg_glucose_level\"], color=shap_values_xgb[:, \"hypertension\"], show=False)\n",
    "plt.title(\"SHAP Scatter plot showing the influence of Average Glucose Level with Hypertension on stroke risk predictions in the XGBoost model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap_values_cat[:, \"avg_glucose_level\"], color=shap_values_cat[:, \"hypertension\"], show=False)\n",
    "plt.title(\"SHAP Scatter plot showing the influence of Average Glucose Level with Hypertension on stroke risk predictions in the CatBoost model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90",
   "metadata": {},
   "source": [
    "Elevated glucose levels are the dominant stroke risk predictor in the CatBoost model, with hypertension significantly amplifying this effect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {},
   "source": [
    "#### Analysis of Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap_values_xgb[:, \"gender\"], show=False)\n",
    "plt.title(\"SHAP Scatter plot showing the influence of Gender on stroke risk predictions in the XGBoost model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.scatter(shap_values_cat[:, \"gender\"], show=False)\n",
    "plt.title(\"SHAP Scatter plot showing the influence of Gender on stroke risk predictions in the CatBoost model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94",
   "metadata": {},
   "source": [
    "Female patients demonstrate significantly higher gender SHAP values compared to males, suggesting a strong gender bias in the model's predictions. However, this effect is an artifact of independent analysis; when other features are included, gender contributes minimally to the overall stroke risk assessment, as shown in the plot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 17\n",
    "\n",
    "shap.plots.waterfall(shap_values_xgb[idx], show=False)\n",
    "plt.title(\"SHAP Waterfall plot for XGBoost stroke prediction model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96",
   "metadata": {},
   "source": [
    "While LIME provides local approximations of model behavior using linear surrogate models, SHAP offers explanations grounded in cooperative game theory. SHAP explanations are additive, consistent, and remain stable across different runs. In contrast to LIME, SHAP provides both global and local interpretability without requiring repeated sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97",
   "metadata": {},
   "source": [
    "# LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install lime -q\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "# Setting up LIME\n",
    "feature_names = X_train_res.columns.tolist()\n",
    "class_names = ['No Stroke', 'Stroke']\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "    X_train_res.values,\n",
    "    feature_names=feature_names,\n",
    "    class_names=class_names,\n",
    "    discretize_continuous=True\n",
    ")\n",
    "\n",
    "# We'll analyze the same instances as the rest of the notebook\n",
    "instance_50 = X_test.iloc[49].values\n",
    "instance_100 = X_test.iloc[99].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99",
   "metadata": {},
   "source": [
    "## Implementing LIME on RF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {},
   "source": [
    "> âš ï¸ Graphs key: Red pushes to No Stroke and Green to Stroke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instance 50\n",
    "print(\"Instance 50\")\n",
    "print(\"Predicted:\", class_names[rf.predict(pd.DataFrame(instance_50.reshape(1, -1), columns=X_train_res.columns))[0]])\n",
    "print(\"Actual:\", class_names[y_test.iloc[49]])\n",
    "exp_rf_50 = explainer.explain_instance(instance_50, rf.predict_proba, num_features=len(feature_names))\n",
    "exp_rf_50.as_pyplot_figure()\n",
    "plt.show()\n",
    "\n",
    "# Instance 100\n",
    "print(\"Instance 100\")\n",
    "print(\"Predicted:\", class_names[rf.predict(pd.DataFrame(instance_100.reshape(1, -1), columns=X_train_res.columns))[0]])\n",
    "print(\"Actual:\", class_names[y_test.iloc[99]])\n",
    "exp_rf_100 = explainer.explain_instance(instance_100, rf.predict_proba, num_features=len(feature_names))\n",
    "exp_rf_100.as_pyplot_figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102",
   "metadata": {},
   "source": [
    "> Results\n",
    "\n",
    "Instance 50 (prediction: No Stroke):\n",
    "  - Strong protective (red): avg_glucose_level â‰¤ ~81.5 and bmi â‰¤ ~25.4 reduce risk.\n",
    "  - Mild risk (green): age in ~41â€“59 and some work_type effect add small positive pushes.\n",
    "  - Reading: Protective signals dominate overall, so the model predicts No Stroke.\n",
    "\n",
    "Instance 100 (prediction: Stroke):\n",
    "  - Dominant risk (green): age in ~59.5â€“74.5, work_type threshold, and higher avg_glucose_level.\n",
    "  - Local counterâ€‘effect (red): a BMI threshold can appear protective here due to local interactions; this does not imply BMI is globally protective.\n",
    "  - Reading: Age and glucose are the main drivers flipping the prediction to Stroke; other features reinforce the decision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103",
   "metadata": {},
   "source": [
    "## Implementing LIME of XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instance 50\n",
    "print(\"Instance 50\")\n",
    "print(\"Predicted:\", class_names[xgb.predict(pd.DataFrame(instance_50.reshape(1, -1), columns=X_train_res.columns))[0]])\n",
    "print(\"Actual:\", class_names[y_test.iloc[49]])\n",
    "exp_xgb_50 = explainer.explain_instance(instance_50, xgb.predict_proba, num_features=len(feature_names))\n",
    "exp_xgb_50.as_pyplot_figure()\n",
    "plt.show()\n",
    "\n",
    "# Instance 100\n",
    "print(\"Instance 100\")\n",
    "print(\"Predicted:\", class_names[xgb.predict(pd.DataFrame(instance_100.reshape(1, -1), columns=X_train_res.columns))[0]])\n",
    "print(\"Actual:\", class_names[y_test.iloc[99]])\n",
    "exp_xgb_100 = explainer.explain_instance(instance_100, xgb.predict_proba, num_features=len(feature_names))\n",
    "exp_xgb_100.as_pyplot_figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105",
   "metadata": {},
   "source": [
    "> Results\n",
    "\n",
    "Instance 50 (predicted: No Stroke)\n",
    "\n",
    "- avg_glucose_level â‰¤ ~81.5 is strongly protective.\n",
    "- bmi â‰¤ ~25.4 also reduces risk.\n",
    "- age in ~41â€“59 adds some local risk.\n",
    "- work_type threshold contributes a smaller positive push.\n",
    "Takeaway: Low glucose and normal BMI dominate, outweighing moderate age/work-related risk, so the model stays with No Stroke.\n",
    "\n",
    "Instance 100 (predicted: Stroke)\n",
    "- age in ~59.5â€“74.5 is the largest driver toward Stroke.\n",
    "- work_type threshold contributes meaningful positive risk.\n",
    "- avg_glucose_level > ~170 adds further risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106",
   "metadata": {},
   "source": [
    "> Post Analysis Note:\n",
    "\n",
    "bmi > ~31.8 shows a negative (red) contribution here; that can happen locally due to feature interactions and how LIME bins features. It doesnâ€™t mean high BMI is globally protective, more so just that, for this patientâ€™s mix of values, BMI is not the main driver of the Stroke prediction.\n",
    "\n",
    "Takeaway: Age and high glucose chiefly flip the prediction to Stroke; other features reinforce the direction.\n",
    "\n",
    "- Practical note:\n",
    "  - Prefer medically plausible patterns (older age, higher glucose increasing risk). If LIME highlights implausible drivers, revisit preprocessing (e.g., outlier handling) or constraints.\n",
    "\n",
    "- Reminder:\n",
    "  - LIME is local to the chosen instance and explains â€œwhy this prediction here,â€ not global importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107",
   "metadata": {},
   "source": [
    "## Interactive Widget for all Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntSlider, Dropdown, HBox, VBox, Output\n",
    "from IPython.display import display\n",
    "\n",
    "try:\n",
    "    viewer_container.close()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Controls\n",
    "idx_slider = IntSlider(value=0, min=0, max=len(X_test)-1, step=1, description='Instance')\n",
    "model_picker = Dropdown(options=[('Random Forest', 'rf'), ('XGBoost', 'xgb')], value='rf', description='Model')\n",
    "\n",
    "# Single output area\n",
    "out = Output()\n",
    "\n",
    "# Render once per change into the single output area\n",
    "def render(change=None):\n",
    "    with out:\n",
    "        out.clear_output(wait=True)\n",
    "        idx = idx_slider.value\n",
    "        model_key = model_picker.value\n",
    "        model = rf if model_key == 'rf' else xgb\n",
    "        x = X_test.iloc[idx].values\n",
    "\n",
    "        # Predicted/Actual labels\n",
    "        pred_label = model.predict(pd.DataFrame(x.reshape(1, -1), columns=X_train_res.columns))[0]\n",
    "        actual_label = y_test.iloc[idx]\n",
    "        print(f\"Instance {idx}\")\n",
    "        print(\"Predicted:\", class_names[pred_label])\n",
    "        print(\"Actual:\", class_names[actual_label])\n",
    "\n",
    "        # Instance values\n",
    "        try:\n",
    "            decoded = transformCategory(X_test.iloc[[idx]])\n",
    "            print(\"\\nInstance values:\")\n",
    "            display(decoded)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # LIME explanation\n",
    "        exp = explainer.explain_instance(\n",
    "            x,\n",
    "            lambda d: model.predict_proba(pd.DataFrame(d, columns=feature_names)),\n",
    "            num_features=len(feature_names)\n",
    "        )\n",
    "        exp.as_pyplot_figure()\n",
    "        plt.show()\n",
    "\n",
    "        print(\"\\nVaraibles and their influence:\")\n",
    "        for rule, weight in exp.as_list():\n",
    "            print(f\"- {rule} â†’ {weight:.3f}\")\n",
    "\n",
    "# Hook up observers (only once, for this container)\n",
    "idx_slider.observe(render, names='value')\n",
    "model_picker.observe(render, names='value')\n",
    "\n",
    "# Display container and do initial render\n",
    "viewer_container = VBox([HBox([idx_slider, model_picker]), out])\n",
    "display(viewer_container)\n",
    "render()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109",
   "metadata": {},
   "source": [
    "## SP-LIME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110",
   "metadata": {},
   "source": [
    "Up until now, our analysis has been more local and not necessarily representative of the overall model. In the following section, with a motivation to garner a more global analysis, we will implement SP-LIME, \"a method that selects a set of representative instances with explanations to address the â€œtrusting the model problem, via submodular optimization.\" [1]\n",
    "\n",
    "> [1] Quote from Ribeiro, M. T., Singh, S., & Guestrin, C. (2016). \"Why Should I Trust You?\": Explaining the Predictions of Any Classifier. ArXiv. https://arxiv.org/abs/1602.04938"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SP-LIME Implementation, submodular pick for global and representative explanations for RF & XGB\n",
    "N = len(X_test)\n",
    "D = X_test.iloc[:N].values\n",
    "\n",
    "# RF\n",
    "sp_rf = SubmodularPick(\n",
    "    explainer,\n",
    "    D,\n",
    "    lambda d: rf.predict_proba(pd.DataFrame(d, columns=feature_names)),\n",
    "    num_features=10,\n",
    "    num_exps_desired=8\n",
    ")\n",
    "\n",
    "# XGB\n",
    "sp_xgb = SubmodularPick(\n",
    "    explainer,\n",
    "    D,\n",
    "    lambda d: xgb.predict_proba(pd.DataFrame(d, columns=feature_names)),\n",
    "    num_features=10,\n",
    "    num_exps_desired=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the SP-LIME representative instances\n",
    "\n",
    "rf_positions = list(getattr(sp_rf, 'V', []))\n",
    "xgb_positions = list(getattr(sp_xgb, 'V', []))\n",
    "\n",
    "print(\"SP-LIME RF representative instances:\")\n",
    "print(f\"Positions in X_test: {rf_positions}\")\n",
    "display(X_test.iloc[rf_positions].reset_index(drop=True))\n",
    "\n",
    "print(\"SP-LIME XGB representative instances:\")\n",
    "print(f\"Positions in X_test: {xgb_positions}\")\n",
    "display(X_test.iloc[xgb_positions].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113",
   "metadata": {},
   "source": [
    "> Insights\n",
    "\n",
    "Submodular Pick (SP-LIME) selected a small, representative set of instances from `X_test` that best covers the diversity of local LIME explanations. In our case, we have generated 8 such instances, displayed above. The motivation stems from the fact that instance-by-instance LIME, what we did in the last section, is local. Here, SP-LIME lifts it to a global view without scanning the entire dataset.\n",
    "\n",
    "> Conclusions:\n",
    "\n",
    "- Across both RF and XGB, age and avg_glucose_level consistently appear as top drivers, with bmi and work_type also influential. \n",
    "- Cardiovascular indicators (hypertension, heart_disease) contribute but with smaller average weights than age/glucose. \n",
    "- The representative subset reflects diverse categorical combinations (gender, residence, marriage), which helps capture different decision regions. Decoding confirms categories are sensible and interpretable (no unexpected codes). This improves the trust and use of instance level reviews for clinicians and stakeholders.\n",
    "\n",
    "\n",
    "> What the results show:\n",
    "\n",
    "- Consistent global drivers across models:\n",
    "  - Age is the strongest contributor in both RF and XGB.\n",
    "  - Avg glucose level and BMI are also influential, with work_type showing moderate importance.\n",
    "- Additional, smaller contributors:\n",
    "  - Residence_type and smoking_status have non-trivial but smaller average impacts.\n",
    "  - Binary comorbids (hypertension, heart_disease) contribute less on average globally, but can be decisive for specific individuals.\n",
    "- Local variability:\n",
    "  - Across the representative set, feature importance patterns vary by instance, reinforcing that LIME explanations are local.\n",
    "  - Previously observed LIME fidelity scores (exp.score) and distributions indicate decent but instance-dependent surrogate fit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114",
   "metadata": {},
   "source": [
    "Below we will look at two metrics that help us understand the LIME analysis in a more quantitative way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115",
   "metadata": {},
   "source": [
    "## LIME Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116",
   "metadata": {},
   "source": [
    "### LIME Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average absolute LIME weight for RF\n",
    "N = len(X_test)\n",
    "weights = pd.Series(0.0, index=feature_names)\n",
    "counts = pd.Series(0, index=feature_names)\n",
    "\n",
    "for idx in range(N):\n",
    "    x = X_test.iloc[idx].values\n",
    "    exp = explainer.explain_instance(\n",
    "        x,\n",
    "        lambda d: rf.predict_proba(pd.DataFrame(d, columns=feature_names)),\n",
    "        num_features=10\n",
    "    )\n",
    "    for rule, w in exp.as_list():\n",
    "        fname = rule.split(' ')[0]\n",
    "        if fname in weights.index:\n",
    "            weights[fname] += abs(w)\n",
    "            counts[fname] += 1\n",
    "\n",
    "avg_abs = (weights / counts.replace(0, np.nan)).dropna().sort_values(ascending=False).head(10)\n",
    "plt.figure(figsize=(8, 4))\n",
    "avg_abs.plot(kind='barh', color='teal')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('RF: Avg absolute LIME weight')\n",
    "plt.xlabel('|weight|')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average absolute LIME weight per feature for XGB\n",
    "\n",
    "N = len(X_test)\n",
    "weights = pd.Series(0.0, index=feature_names)\n",
    "counts = pd.Series(0, index=feature_names)\n",
    "\n",
    "for idx in range(N):\n",
    "    x = X_test.iloc[idx].values\n",
    "    exp = explainer.explain_instance(\n",
    "        x,\n",
    "        lambda d: xgb.predict_proba(pd.DataFrame(d, columns=feature_names)),\n",
    "        num_features=10\n",
    "    )\n",
    "    for rule, w in exp.as_list():\n",
    "        fname = rule.split(' ')[0]\n",
    "        if fname in weights.index:\n",
    "            weights[fname] += abs(w)\n",
    "            counts[fname] += 1\n",
    "\n",
    "avg_abs = (weights / counts.replace(0, np.nan)).dropna().sort_values(ascending=False).head(10)\n",
    "plt.figure(figsize=(8, 4))\n",
    "avg_abs.plot(kind='barh', color='purple')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('XGB: Avg absolute LIME weight')\n",
    "plt.xlabel('|weight|')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119",
   "metadata": {},
   "source": [
    "Average absolute LIME weights tells us how strongly a feature influences the model's prediction across many local LIME explanations. We take the absolute value of the weight bec in this case we are more interested in the influence, regardless of whether it is positive or negative. This allows us to then average values across many instances and understand which features are typically most impactful locally.\n",
    "\n",
    "> Insights\n",
    "\n",
    "In both models, the top two features with the most impact are the the same, namely, age and average glucose level. Comparing this with scientific literature, this checks out. These results go a long way into building trust on the model. \n",
    "\n",
    "However, at the same time, something suspicious is that prior hypertension and heart disease existence has almost no influence (<0.1) on the prediction. As a reminder, we remember that these magnitudes are local, and they depend on LIME binning. They should be used together with fidelity (see below)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120",
   "metadata": {},
   "source": [
    "### LIME Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIME fidelity distribution (RF vs XGB) with counts\n",
    "\n",
    "N = min(300, len(X_test))\n",
    "rf_scores, xgb_scores = [], []\n",
    "\n",
    "for idx in range(N):\n",
    "    x = X_test.iloc[idx].values\n",
    "    exp_rf = explainer.explain_instance(\n",
    "        x,\n",
    "        lambda d: rf.predict_proba(pd.DataFrame(d, columns=feature_names)),\n",
    "        num_features=10\n",
    "    )\n",
    "    rf_scores.append(getattr(exp_rf, 'score', np.nan))\n",
    "\n",
    "    exp_xgb = explainer.explain_instance(\n",
    "        x,\n",
    "        lambda d: xgb.predict_proba(pd.DataFrame(d, columns=feature_names)),\n",
    "        num_features=10\n",
    "    )\n",
    "    xgb_scores.append(getattr(exp_xgb, 'score', np.nan))\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "counts_list, bins, containers = plt.hist([rf_scores, xgb_scores], bins=20, label=['RF', 'XGB'], alpha=0.7)\n",
    "plt.title('LIME fidelity (local surrogate R^2) distribution')\n",
    "plt.xlabel('R^2 (score)')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "\n",
    "# Annotation of counts\n",
    "for counts, container in zip(counts_list, containers):\n",
    "    for c, rect in zip(counts, container.patches):\n",
    "        if c > 0:\n",
    "            x = rect.get_x() + rect.get_width() / 2\n",
    "            y = rect.get_height()\n",
    "            plt.text(x, y + max(0.5, y * 0.02), str(int(round(c))), ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122",
   "metadata": {},
   "source": [
    "This shows us the distribution of the local surrogate RÂ² (fidelity) for RF and XGB. For the right cluster i.e. R^2 of ~0.5-0.7 or so, both models perform fairly well. In this cluster, we can trust both the direction and the magnitude of the feature weights.\n",
    "\n",
    "For the left cluster, we have low fidelity scores. It tells us that the local linear surrogate struggles to explain the predictions. We can only use LIME for a rough directional hint here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probability vs LIME fidelity (RF and XGB)\n",
    "\n",
    "N = min(300, len(X_test))\n",
    "rf_probs, xgb_probs = [], []\n",
    "rf_scores, xgb_scores = [], []\n",
    "\n",
    "for idx in range(N):\n",
    "    x = X_test.iloc[idx].values\n",
    "    df = pd.DataFrame(x.reshape(1, -1), columns=feature_names)\n",
    "\n",
    "    # model probabilities\n",
    "    rf_probs.append(rf.predict_proba(df)[0, 1])\n",
    "    xgb_probs.append(xgb.predict_proba(df)[0, 1])\n",
    "\n",
    "    # LIME fidelity (local surrogate R^2)\n",
    "    exp_rf = explainer.explain_instance(\n",
    "        x,\n",
    "        lambda d: rf.predict_proba(pd.DataFrame(d, columns=feature_names)),\n",
    "        num_features=10\n",
    "    )\n",
    "    rf_scores.append(getattr(exp_rf, 'score', np.nan))\n",
    "\n",
    "    exp_xgb = explainer.explain_instance(\n",
    "        x,\n",
    "        lambda d: xgb.predict_proba(pd.DataFrame(d, columns=feature_names)),\n",
    "        num_features=10\n",
    "    )\n",
    "    xgb_scores.append(getattr(exp_xgb, 'score', np.nan))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n",
    "axes[0].scatter(rf_probs, rf_scores, alpha=0.6, color='skyblue', s=18)\n",
    "axes[0].set_title('RF: prob vs LIME fidelity')\n",
    "axes[0].set_xlabel('Predicted stroke probability')\n",
    "axes[0].set_ylabel('LIME fidelity (R^2)')\n",
    "\n",
    "axes[1].scatter(xgb_probs, xgb_scores, alpha=0.6, color='orange', s=18)\n",
    "axes[1].set_title('XGB: prob vs LIME fidelity')\n",
    "axes[1].set_xlabel('Predicted stroke probability')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124",
   "metadata": {},
   "source": [
    "> Predicted probability vs LIME fidelity â€” insights\n",
    "\n",
    "- RF:\n",
    "    - We see two clusterings, a higherâ€‘fidelity band around ~0.55â€“0.65 (mostly at lower risk) and a lower band around ~0.1â€“0.25 across the rest.\n",
    "    - This suggests RFâ€™s decisions are easier to approximate locally for many lowâ€‘risk predictions, while midâ€‘toâ€‘high risk cases often look more complex (lower RÂ²).\n",
    "- XGB:\n",
    "    - More spread and generally lower fidelity on average than RF, with many points clustered at low RÂ² (â‰ˆ0.05â€“0.2) across the full probability range.\n",
    "    - This hints XGBâ€™s decision boundary is more nonlinear/complex, making a simple local surrogate (like LIMEâ€™s linear model) fit less tightly.\n",
    "\n",
    "> Fidelity:\n",
    "\n",
    "Fidelity is a trust measure for local explanations. It tells us how well the LIME surrogate model represents the original model's outputs on the samples. A high fidelity i.e. close to 1 tells us that the surrogate matches the original model fairly well and vice versa. \n",
    "\n",
    "With a high fidelity i.e. >0.6, we can trust both the direction and the relative magnitude of the LIME weights. A moderate fidelity i.e. 0.3-0.6, we should treat LIME as a mere directional insight i.e. what features push the predictions up/down. As for low fidelity i.e. <0.3 we should realize that the local linear surrogate does not do a very good job of fitting to the model. \n",
    "\n",
    "In this case, in the cluster of High RÂ², we can treat the LIME weights as a more faithful picture of the modelâ€™s local behavior. Meanwhile for Low RÂ², we use LIME weights for rough influence measure i.e. which features push up or down risk. Again, this is not a precise magnitude, thus results should be cross checked with a professional for instance.\n",
    "\n",
    "> Conclusions:\n",
    "\n",
    "We can see that on average that our LIME implementation does a better job at approximating to the RF compared to XGB. \n",
    "What next? this can be studied and compared with other datasets to see whether LIME works better with RF as compared to XGB. If this is indeed the case then this will help us contribute towards the open field of xAI research.\n",
    "Moreover, after this visualization, we can tell the doctors which model to trust more depending on whether they want to prioritize recall or precision in their predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125",
   "metadata": {},
   "source": [
    "## Stats of all Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(X_test)\n",
    "\n",
    "rf_probs, xgb_probs = [], []\n",
    "rf_scores, xgb_scores = [], []\n",
    "\n",
    "for idx in range(N):\n",
    "    x = X_test.iloc[idx].values\n",
    "    df = pd.DataFrame(x.reshape(1, -1), columns=feature_names)\n",
    "    \n",
    "    # Original model probabilities\n",
    "    rf_probs.append(rf.predict_proba(df)[0, 1])\n",
    "    xgb_probs.append(xgb.predict_proba(df)[0, 1])\n",
    "    \n",
    "    # Comparing local surrogate fidelity with the model\n",
    "    exp_rf = explainer.explain_instance(\n",
    "        x, \n",
    "        lambda d: rf.predict_proba(pd.DataFrame(d, columns=feature_names)),\n",
    "        num_features=5  # fewer features for speed\n",
    "    )\n",
    "    rf_scores.append(getattr(exp_rf, 'score', np.nan))\n",
    "    \n",
    "    exp_xgb = explainer.explain_instance(\n",
    "        x, \n",
    "        lambda d: xgb.predict_proba(pd.DataFrame(d, columns=feature_names)),\n",
    "        num_features=10\n",
    "    )\n",
    "    xgb_scores.append(getattr(exp_xgb, 'score', np.nan))\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    'model': ['Random Forest', 'XGBoost'],\n",
    "    'proba_mean': [np.mean(rf_probs), np.mean(xgb_probs)],\n",
    "    'proba_median': [np.median(rf_probs), np.median(xgb_probs)],\n",
    "    'proba_std': [np.std(rf_probs), np.std(xgb_probs)],\n",
    "    'lime_fidelity_mean': [np.nanmean(rf_scores), np.nanmean(xgb_scores)],\n",
    "    'lime_fidelity_median': [np.nanmedian(rf_scores), np.nanmedian(xgb_scores)]\n",
    "})\n",
    "\n",
    "print(f\"Sample size: {N}\")\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127",
   "metadata": {},
   "source": [
    "> What the cell computes\n",
    "\n",
    "- For each test instance, it collects:\n",
    "  - Predicted stroke probabilities from RF and XGB (rf_probs, xgb_probs).\n",
    "  - LIME surrogate fidelity scores for RF and XGB (rf_scores, xgb_scores), i.e., how well the local linear model approximates the black-box around that instance.\n",
    "\n",
    "> Interpreting the results\n",
    "\n",
    "- Stroke probability (original model outputs)\n",
    "  - XGBoost shows higher mean/median stroke probability than Random Forest. This essentailly tells us XGBoost is more riskâ€‘sensitive and will flag more patients as likely stroke. This is good! Recall over Precision is best in medical fields\n",
    "  - XGBoostâ€™s probabilities are also more spread out (higher variability), while Random Forest looks more conservative/stable. Expect XGBoost to boost recall but potentially lower precision.\n",
    "\n",
    "- LIME fidelity (how well the local surrogate mimics the model)\n",
    "  - Average fidelity is modest (roughly in the 0.27â€“0.33 range, while the median is lower). Normal for fast, lightweight settings and reminds us LIME is an approximation.\n",
    "  - Use LIME primarily for the direction and main drivers (which features push toward Stroke vs No Stroke). Avoid overâ€‘interpreting tiny weight differences.\n",
    "  - We are aware that fidelity can be improved by adjusting kernel width, or allowing more features in the local surrogate, but as far as analysis has to go, this suffices + kept it small for speed\n",
    "\n",
    "- The mean/median of rf_probs vs xgb_probs show how conservative or aggressive each model is. Higher averages imply the model is assigning more cases near the decision threshold.\n",
    "- LIME fidelity (exp.score):\n",
    "  - Higher average fidelity implies explanations more faithfully capture local behavior. Low-fidelity instances deserve caution; the local rules may be unstable or sensitive to perturbations.\n",
    "  - In our case, RF has a higher fidelity score implying that its local explanations are more reliable for decision support.\n",
    "\n",
    "> How to use this?\n",
    "- We should prefer the model that achieves acceptable probability calibration and higher average LIME fidelity in cohorts of interest. If RF is more faithful locally for older patients, doctors should use RF-driven explanations for that subgroup.\n",
    "- Having low fidelity does not mean we should discard the LIME aproximation altogether; rather we should flag low fidelity instances for additional review. For these cases, we should avoid relying solely on local rules; consider global checks or alternative explanation methods (SHAP etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128",
   "metadata": {},
   "source": [
    "## Conclusions with Cohort-Wise Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cohort-wise stats (simple bar charts)\n",
    "\n",
    "if 'results' not in globals() or not isinstance(results, dict) or len(results) == 0:\n",
    "    print(\"[warn] No cohort results found. Run the cohort-wise stats cell first.\")\n",
    "else:\n",
    "    for cohort_key, dfc in results.items():\n",
    "        if not isinstance(dfc, pd.DataFrame) or dfc.empty:\n",
    "            print(f\"[info] Empty results for cohort '{cohort_key}', skipping.\")\n",
    "            continue\n",
    "\n",
    "        dfc = dfc.sort_index()\n",
    "\n",
    "        # Observed vs predicted mean probabilities\n",
    "        comp = pd.DataFrame({\n",
    "            'Observed rate': dfc['pos_rate'],\n",
    "            'RF mean prob': dfc['rf_prob_mean'],\n",
    "            'XGB mean prob': dfc['xgb_prob_mean'],\n",
    "        })\n",
    "        ax = comp.plot(kind='bar', figsize=(8, 4))\n",
    "        ax.set_title(f\"{cohort_key}: observed vs predicted mean probability\")\n",
    "        ax.set_ylabel(\"Probability\")\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.legend(loc='best')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Average LIME fidelity\n",
    "        fid = pd.DataFrame({\n",
    "            'RF fidelity': dfc['rf_fidelity_mean'],\n",
    "            'XGB fidelity': dfc['xgb_fidelity_mean'],\n",
    "        })\n",
    "        ax = fid.plot(kind='bar', figsize=(8, 4))\n",
    "        ax.set_title(f\"{cohort_key}: average LIME fidelity\")\n",
    "        ax.set_ylabel(\"LIME exp.score\")\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.legend(loc='best')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Calibration gap bars (mean prob - observed)\n",
    "        gap = pd.DataFrame({\n",
    "            'RF gap': dfc['rf_calibration_gap'],\n",
    "            'XGB gap': dfc['xgb_calibration_gap'],\n",
    "        })\n",
    "        ax = gap.plot(kind='bar', figsize=(8, 3))\n",
    "        ax.axhline(0, color='black', linewidth=0.8)\n",
    "        ax.set_title(f\"{cohort_key}: calibration gap (mean prob - observed)\")\n",
    "        ax.set_ylabel(\"Gap\")\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.legend(loc='best')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130",
   "metadata": {},
   "source": [
    "### Cohort-wise Statistics Summary and Analysis\n",
    "\n",
    "> Motivation\n",
    "\n",
    "Stroke risk is not uniform across populations. Age, cardiovascular health, hypertension, and lifestyle factors (smoking) are well-established risk modifiers. With the above visualizations, our goal is to identify whether our models (Random Forest and XGBoost) and LIME explanations behave consistently across different demographic and clinical groups, or if certain cohorts show:\n",
    "- Calibration problems: models over/underestimate risk for specific groups.\n",
    "- Fidelity issues: LIME explanations are less trustworthy (less faithful) in certain cohorts.\n",
    "- Potential fairness concerns: systematic biases in predictions or unreliable explanations for vulnerable groups.\n",
    "\n",
    "> 1. Observed vs Predicted Mean Probability Chart\n",
    "- X-axis: cohort groups (e.g., age bands <40, 40-60, >60).\n",
    "- Y-axis: probability (0â€“1).\n",
    "- Three bars per group: orange=observed rate, blue=RF mean probability, green=XGB mean probability.\n",
    "\n",
    "Interpretation:\n",
    "- If all three bars align closely, the model is well-calibrated for that cohort.\n",
    "- If RF or XGB bars are consistently above the orange bar, the model is overestimating risk (too aggressive) for that group.\n",
    "- If bars are consistently below, the model is underestimating risk (too conservative) for that group.\n",
    "- Look for patterns: do older patients or those with hypertension see larger discrepancies?\n",
    "\n",
    "Action: Cohorts with large gaps can improve with threshold adjustment or model retraining on that subpopulation.\n",
    "\n",
    "\n",
    "\n",
    "> 2. Average LIME Fidelity Chart\n",
    "- X-axis: cohort groups.\n",
    "- Y-axis: LIME exp.score (0â€“1, where 1 is perfect faithfulness).\n",
    "- Two bars per group: blue=RF fidelity, orange=XGB fidelity.\n",
    "\n",
    "Interpretation:\n",
    "- Fidelity < 0.5: local explanations are unreliable; the linear surrogate model does not capture the decision boundary well.\n",
    "- Fidelity 0.5â€“0.8: moderate reliability; use explanations as a starting point but verify with domain knowledge.\n",
    "- Fidelity > 0.8: high confidence; explanations are trustworthy for decision support.\n",
    "\n",
    "Common patterns:\n",
    "- Cohorts with extreme probabilities (very high or very low risk) typically have higher fidelity because the decision is \"clearer.\" This can help us pin point features that contribute the most to the prediction.\n",
    "- Cohorts with mid-range probabilities (borderline cases) often show lower fidelity because small perturbations can flip the prediction.\n",
    "- If one model (in our case, RF) has consistently higher fidelity, use RF-driven explanations as primary and XGB as secondary.\n",
    "\n",
    "Action: Flag cohorts with low fidelity (<0.5) for manual review. For borderline cases in those cohorts, consider alternative explainability methods.\n",
    "\n",
    "\n",
    "\n",
    "> 3. Calibration Gap Chart\n",
    "- X-axis: cohort groups.\n",
    "- Y-axis: gap = mean predicted probability âˆ’ observed rate.\n",
    "- Two bars per group: blue=RF gap, orange=XGB gap.\n",
    "\n",
    "Interpretation:\n",
    "- Bars above zero: overestimation (model is too confident).\n",
    "- Bars below zero: underestimation (model is too conservative).\n",
    "- Magnitude indicates how far off the model is. A gap of Â±0.1 means the model is off by ~10 percentage points.\n",
    "\n",
    "Examples:\n",
    "- RF for age >60 showing gap of around +0.50 means RF predicts ~35% stroke risk but far less of them will actually have strokes in that group. This is problematic as patients may receive unnecessary interventions.\n",
    "- XGB for smokers showing gap of around 0.3 means XGB predicts ~30% stroke risks but less actually have strokes.\n",
    "\n",
    "Action:\n",
    "- We could use these calibration plots to select thresholds that match clinical cost-benefit trade-offs.\n",
    "- Report gaps to doctors so they understand model limitations per demographic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131",
   "metadata": {},
   "source": [
    "# Counterfactual Explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132",
   "metadata": {},
   "source": [
    "## A glance over Dice \n",
    "DiCE is a library developed by Microsoft that specializes in generating a set of diverse counterfactual examples. The \"diversity\" aspect is crucial because there is rarely just one way to change an outcome. DiCE attempts to find multiple paths to the desired prediction (e.g., changing BMI vs. changing smoking status) while ensuring that the suggested changes are:\n",
    "\n",
    "1/Diversity:\n",
    "\n",
    "    - DiCE doesnâ€™t just generate a single counterfactual; it generates a set of alternatives.\n",
    "    Example: To get a loan approved, one option might be â€œincrease income,â€ another might be â€œreduce debt,â€ and yet another could be â€œimprove credit score.â€\n",
    "\n",
    "-->This helps users choose a path that is practical for them.\n",
    "\n",
    "2/Proximity\n",
    "\n",
    "    - DiCE tries to make counterfactuals close to the original input, so the suggested changes are minimal and realistic.\n",
    "    Example: If your BMI is 30, suggesting a BMI of 18 might technically change the outcome but is too drastic. DiCE prefers smaller, feasible changes.\n",
    "\n",
    "--> Ensure reasonable values\n",
    "\n",
    "3/Feasibility\n",
    "\n",
    "    - DiCE ensures counterfactuals make sense according to the data.\n",
    "    For example, it wonâ€™t suggest â€œage = -5â€ or â€œmarried = yes AND single = yes.â€\n",
    "\n",
    "-->This is done by respecting constraints and distributions in the training data.\n",
    "\n",
    "4/Flexibility\n",
    "\n",
    "    - DiCE can be used with any predictive model (tree-based, neural networks, etc.) because it works with the modelâ€™s predictions, not its internal workings.\n",
    "    - Handles **continuous, categorical, and ordinal features**.  \n",
    "\n",
    "\n",
    "Note: DiCE works best with **scikit-learn** and **TensorFlow**. Other frameworks may require additional adjustments to work correctly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133",
   "metadata": {},
   "source": [
    "## Explainable Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_maps = {\n",
    "    'gender': {0: 'Female', 1: 'Male', 2: 'Other'},\n",
    "    'ever_married': {0: 'No', 1: 'Yes'},\n",
    "    'work_type': {0: 'Govt_job', 1: 'Never_worked', 2: 'Private', 3: 'Self-employed', 4: 'children'},\n",
    "    'Residence_type': {0: 'Rural', 1: 'Urban'},\n",
    "    'smoking_status': {0: 'Unknown', 1: 'formerly smoked', 2: 'never smoked', 3: 'smokes'}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Map the catetory to human-readable\n",
    "def transformCategory(df):\n",
    "    df_copy = df.copy()  \n",
    "    for col, mapping in categorical_maps.items():\n",
    "        if col in df_copy.columns:  \n",
    "            df_copy[col] = df_copy[col].map(mapping)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136",
   "metadata": {},
   "source": [
    "### Generating CEs using DiCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137",
   "metadata": {},
   "source": [
    "### For Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install dice-ml -q\n",
    "import dice_ml\n",
    "\n",
    "categorical_features = ['gender', 'hypertension', 'heart_disease',\n",
    "                        'ever_married', 'work_type', 'Residence_type', 'smoking_status']\n",
    "\n",
    "continuous_features = ['age', 'avg_glucose_level', 'bmi']\n",
    "\n",
    "d = dice_ml.Data(\n",
    "    dataframe=X_train_res.assign(target=y_train_res), \n",
    "    continuous_features=continuous_features,\n",
    "    outcome_name='target'\n",
    ")\n",
    "\n",
    "m = dice_ml.Model(model=rf, backend='sklearn')  \n",
    "exp = dice_ml.Dice(d, m, method='random')  # method to generate CFs\n",
    "\n",
    "\n",
    "query_instance = X_test.loc[[4858]].copy() # Can change this to different instance\n",
    "   \n",
    "\n",
    "cf = exp.generate_counterfactuals(\n",
    "    query_instance,\n",
    "    total_CFs=3,\n",
    "    desired_class=\"opposite\",\n",
    ")\n",
    "\n",
    "cf_df = cf.cf_examples_list[0].final_cfs_df  \n",
    "\n",
    "transformCategory(cf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_target = rf.predict(query_instance)  \n",
    "query_instance.loc[:, \"target\"] = predicted_target\n",
    "transformCategory(query_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140",
   "metadata": {},
   "source": [
    "### So this works totally fine because DiCE supports directly sklearn and TF. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141",
   "metadata": {},
   "source": [
    "### For XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install dice-ml -q\n",
    "import dice_ml\n",
    "\n",
    "categorical_features = ['gender', 'hypertension', 'heart_disease',\n",
    "                        'ever_married', 'work_type', 'Residence_type', 'smoking_status']\n",
    "\n",
    "continuous_features = ['age', 'avg_glucose_level', 'bmi']\n",
    "\n",
    "d = dice_ml.Data(\n",
    "    dataframe=X_train_res.assign(target=y_train_res), \n",
    "    continuous_features=continuous_features+categorical_features,\n",
    "    outcome_name='target'\n",
    ")\n",
    "\n",
    "m = dice_ml.Model(model=xgb, backend='sklearn')  \n",
    "exp = dice_ml.Dice(d, m, method='random')  \n",
    "\n",
    "\n",
    "query_instance = X_test.loc[[196]].copy()\n",
    "   \n",
    "\n",
    "cf = exp.generate_counterfactuals(\n",
    "    query_instance,\n",
    "    total_CFs=3,\n",
    "    desired_class=\"opposite\",\n",
    ")\n",
    "\n",
    "cf_df = cf.cf_examples_list[0].final_cfs_df  \n",
    "\n",
    "transformCategory(cf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_target = xgb.predict(query_instance)  \n",
    "query_instance.loc[:, \"target\"] = predicted_target\n",
    "transformCategory(query_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144",
   "metadata": {},
   "source": [
    "### Identified Issue and Resolution\n",
    "\n",
    "During the analysis, an unexpected issue was encountered due to a mismatch between DiCE and XGBoost. Although all feature columns were correctly defined with numerical data types (int and float) in the original dataset, DiCE internally converts some features to object type during counterfactual generation. This behavior leads to a conflict, as XGBoost does not accept object-typed inputs, resulting in runtime errors.\n",
    "\n",
    "After investigation, two possible solutions were identified:\n",
    "\n",
    "1/ Using the kdtree method\n",
    "This approach works reliably because it generates counterfactuals by copying existing instances from the dataset rather than perturbing feature values. As a result, the original data types are preserved, preventing any type mismatch between DiCE and XGBoost. This method fully resolves the issue without side effects.\n",
    "\n",
    "2/ Treating categorical features as continuous features\n",
    "An alternative (though less principled) solution is to include all categorical features in the continuous_features list and treat them as numerical values. While this approach technically resolves the data type conflict and works well with most DiCE methods, it must be used with caution. In particular, when using the genetic algorithm or Random , this setup may produce unrealistic or invalid categorical values due to numerical perturbations. However, Random method, this issue was not observed, and the categorical values remained valid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145",
   "metadata": {},
   "source": [
    "### For CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install dice-ml -q\n",
    "import dice_ml\n",
    "\n",
    "categorical_features = ['gender', 'hypertension', 'heart_disease',\n",
    "                        'ever_married', 'work_type', 'Residence_type', 'smoking_status']\n",
    "\n",
    "continuous_features = ['age', 'avg_glucose_level', 'bmi']\n",
    "\n",
    "d = dice_ml.Data(\n",
    "    dataframe=X_train_res.assign(target=y_train_res), \n",
    "    continuous_features=continuous_features+categorical_features,\n",
    "    outcome_name='target'\n",
    ")\n",
    "\n",
    "m = dice_ml.Model(model=catboost_model, backend='sklearn')  \n",
    "exp = dice_ml.Dice(d, m, method='kdtree')  \n",
    "\n",
    "\n",
    "query_instance = X_test.iloc[99:100].copy() # Can change this \n",
    "   \n",
    "\n",
    "cf = exp.generate_counterfactuals(\n",
    "    query_instance,\n",
    "    total_CFs=3,\n",
    "    desired_class=\"opposite\",\n",
    ")\n",
    "\n",
    "cf_df = cf.cf_examples_list[0].final_cfs_df  \n",
    "\n",
    "transformCategory(cf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_target = catboost_model.predict(query_instance)  \n",
    "query_instance.loc[:, \"target\"] = predicted_target\n",
    "transformCategory(query_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148",
   "metadata": {},
   "source": [
    "### Identified Issue and Resolution\n",
    "\n",
    "A similar issue was observed when using CatBoost with DiCE. During counterfactual generation, DiCE attempts to transform integer categorical values (e.g., 1 of type int) into floating-point values (e.g., 1.0 of type float). This behavior is not permitted by CatBoost, which enforces strict type consistency for categorical features.\n",
    "\n",
    "If categorical features are not included in continuous_features, DiCE automatically converts integer-encoded categories (e.g., 0, 1) into pandas category dtype. However, this also leads to an error in CatBoost, as CatBoost requires all categorical columns to be explicitly listed in cat_features. This results in the following error:\n",
    "\n",
    "\"CatBoostError: features data: pandas.DataFrame column 'hypertension' has dtype 'category' but is not in cat_features list\"\n",
    "\n",
    "Due to these constraints, the only reliable solution is to include categorical features in continuous_features and use the kd_tree method. This method generates counterfactuals by copying existing instances rather than numerically perturbing feature values, thereby preserving valid feature values and avoiding data type conflicts with CatBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149",
   "metadata": {},
   "source": [
    "### In general, DiCE provides robust and reliable support primarily for models built with scikit-learn and TensorFlow. When used with other frameworks such as XGBoost and CatBoost, additional compatibility issues may ariseâ€”particularly related to data type handling for categorical featuresâ€”often requiring workarounds or constrained generation methods (e.g., kd_tree) to ensure correct functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150",
   "metadata": {},
   "source": [
    "## Counterfactual Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151",
   "metadata": {},
   "source": [
    "## CEs for Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152",
   "metadata": {},
   "source": [
    "### 1/\n",
    "\n",
    "Original patient profile:\n",
    "\n",
    "| Gender | Age | Hypertension | Heart Disease | Married | Work Type | Residence | Avg Glucose | BMI  | Smoking Status | Target |\n",
    "|--------|-----|--------------|---------------|---------|-----------|-----------|-------------|------|----------------|--------|\n",
    "| Female | 43  | No           | No            | Yes     | Private   | Urban     | 86.67       | 33.3 | never smoked   | 0      |\n",
    "\n",
    "Counterfactuals generated:\n",
    "\n",
    "| Gender | Age | Hypertension | Heart Disease | Married | Work Type | Residence | Avg Glucose | BMI  | Smoking Status | Target |\n",
    "|--------|-----|--------------|---------------|---------|-----------|-----------|-------------|------|----------------|--------|\n",
    "| Female | 43  | 0            | 0             | Yes     | Private   | Urban     | 110.87      | 33.3 | formerly smoked | 1      |\n",
    "| Female | 43  | 0            | 0             | Yes     | Private   | Urban     | 69.48       | 33.3 | never smoked   | 1      |\n",
    "| Female | 43  | 0            | 0             | Yes     | Private   | Urban     | 60.55       | 33.3 | never smoked   | 1      |\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "1. **Target change:** All counterfactuals flip the predicted stroke from 0 â†’ 1.  \n",
    "2. **Feature changes:** Only the **average glucose level** and **smoking status** vary significantly.  \n",
    "3. **Unrealistic pattern:**  \n",
    "   - Two counterfactuals suggest that **lowering average glucose level (69.48, 60.55)** increases stroke risk.  \n",
    "   - This is **not clinically plausible**, as lower glucose does not increase stroke probability.  \n",
    "\n",
    "### Actionable insight\n",
    "\n",
    "- This highlights that the model may have learned **spurious relationships** from rare cases.  \n",
    "- To generate **meaningful and medically realistic counterfactuals and Model** , extreme or low-probability feature values should be filtered out during analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['stroke'][(data[\"avg_glucose_level\"]<=70) & (data['stroke'] == 1)].count() / data['stroke'][(data[\"avg_glucose_level\"]<=70) & (data['stroke'] == 0)].count() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154",
   "metadata": {},
   "source": [
    "> Go deeper we can see that the case that the patient has **avg_glucose_level <= 70** and has stroke is only **3,7%**. So for more reasonable analysis, we should consider those as outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155",
   "metadata": {},
   "source": [
    "### 2/\n",
    "\n",
    "Original patient profile:\n",
    "\n",
    "| Gender | Age | Hypertension | Heart Disease | Married | Work Type | Residence | Avg Glucose | BMI  | Smoking Status | Target |\n",
    "|--------|-----|--------------|---------------|---------|-----------|-----------|-------------|------|----------------|--------|\n",
    "| Male   | 17  | No           | No            | No      | Govt_job  | Urban     | 68.91       | 23.0 | Unknown        | 0      |\n",
    "\n",
    "Counterfactuals generated:\n",
    "\n",
    "| Gender | Age  | Hypertension | Heart Disease | Married | Work Type | Residence | Avg Glucose | BMI  | Smoking Status | Target |\n",
    "|--------|------|--------------|---------------|---------|-----------|-----------|-------------|------|----------------|--------|\n",
    "| Male   | 76.6 | 0            | 0             | No      | Govt_job  | Urban     | 222.16      | 23.0 | Unknown        | 1      |\n",
    "| Male   | 57.1 | 0            | 0             | No      | Govt_job  | Urban     | 179.49      | 23.0 | Unknown        | 1      |\n",
    "| Male   | 65.8 | 0            | 0             | No      | Govt_job  | Urban     | 227.21      | 23.0 | Unknown        | 1      |\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "1. **Target change:** All counterfactuals flip the predicted stroke from 0 â†’ 1.  \n",
    "2. **Feature changes:** The primary changes are in **age** and **average glucose level**, which increase in the CFs.  \n",
    "3. **Reasonable pattern:**  \n",
    "   - Older age and higher glucose levels are associated with increased stroke risk.  \n",
    "   - This aligns with clinical expectations and literature.  \n",
    "\n",
    "### Actionable insight\n",
    "\n",
    "- By removing outliers from the dataset, the counterfactuals now reflect **plausible and medically meaningful scenarios**.  \n",
    "- This demonstrates that careful preprocessing (outlier removal) is essential for **trustworthy counterfactual explanations** in healthcare models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156",
   "metadata": {},
   "source": [
    "### 3/\n",
    "\n",
    "Original patient profile:\n",
    "\n",
    "| Gender | Age | Hypertension | Heart Disease | Married | Work Type | Residence | Avg Glucose | BMI  | Smoking Status | Target |\n",
    "|--------|-----|--------------|---------------|---------|-----------|-----------|-------------|------|----------------|--------|\n",
    "| Female | 5   | No           | No            | No      | children  | Rural     | 102.04      | 18.5 | Unknown        | 0      |\n",
    "\n",
    "Counterfactuals generated:\n",
    "\n",
    "| Gender | Age  | Hypertension | Heart Disease | Married | Work Type   | Residence | Avg Glucose | BMI  | Smoking Status | Target |\n",
    "|--------|------|--------------|---------------|---------|------------|-----------|-------------|------|----------------|--------|\n",
    "| Female | 49.9 | 0            | 0             | No      | Private    | Rural     | 102.04      | 18.5 | Unknown        | 1      |\n",
    "| Female | 74.4 | 0            | 0             | No      | Never_worked | Rural   | 102.04      | 18.5 | Unknown        | 1      |\n",
    "| Female | 76.3 | 0            | 0             | No      | Govt_job   | Rural     | 102.04      | 18.5 | Unknown        | 1      |\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "1. **Target change:** All counterfactuals flip stroke prediction from 0 â†’ 1.  \n",
    "2. **Feature changes:** The main changes are **age** and **work type**, while glucose and BMI remain constant.  \n",
    "3. **Insight:**  \n",
    "   - Even at low BMI and normal glucose levels, older age and occupational stress factors (work type) increase predicted stroke risk.  \n",
    "   - This suggests that **stress-related factors might be important contributors** to stroke risk in the model.  \n",
    "\n",
    "### Actionable insight\n",
    "\n",
    "- Counterfactual analysis highlights that **non-traditional risk factors** such as age-related stress and occupational exposure can strongly influence predicted stroke risk.  \n",
    "- Proper handling of stress and lifestyle factors is crucial, especially for individuals in high-risk age groups.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157",
   "metadata": {},
   "source": [
    "### 4/\n",
    "\n",
    "**Original patient profile:**  \n",
    "\n",
    "| Gender | Age  | Hypertension | Heart Disease | Married | Work Type | Residence | Avg Glucose | BMI  | Smoking Status | Target |\n",
    "|--------|------|--------------|---------------|---------|-----------|-----------|-------------|------|----------------|--------|\n",
    "| Male   | 61.0 | 1            | 1             | Yes     | Govt_job  | Rural     | 86.06       | 34.8 | never smoked   | 1      |\n",
    "\n",
    "**Counterfactuals generated:**  \n",
    "\n",
    "| Gender | Age  | Hypertension | Heart Disease | Married | Work Type | Residence | Avg Glucose | BMI   | Smoking Status | Target |\n",
    "|--------|------|--------------|---------------|---------|-----------|-----------|-------------|-------|----------------|--------|\n",
    "| Male   | 61.0 | 1            | 1             | Yes     | children  | Rural     | 86.06       | 96.1  | never smoked   | 0      |\n",
    "| Male   | 32.9 | 1            | 1             | Yes     | Govt_job  | Rural     | 86.06       | 34.8  | never smoked   | 0      |\n",
    "| Male   | 61.0 | 1            | 1             | Yes     | Govt_job  | Rural     | 67.00       | 34.8  | never smoked   | 0      |\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "1. **Target change:** All counterfactuals flip stroke prediction from 1 â†’ 0.  \n",
    "2. **Feature changes and plausibility:**  \n",
    "   - The most notable change is **BMI = 96.1** in one counterfactual, which is **clinically implausible**.  \n",
    "3. **Insight:**  \n",
    "   DiCE produces implausible clinical cases, e.g., suggesting that higher BMI lowers stroke risk, which contradicts real-world evidence.\n",
    "   \n",
    "### Actionable insight\n",
    "\n",
    "- Counterfactual analysis reveals that **extreme BMI changes dominate the modelâ€™s predictions**, but these are **not realistic**.  \n",
    "- For meaningful interpretation, **filtering out implausible counterfactuals** is necessary.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[(data['bmi']>=50) & (data['stroke']==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[(data['bmi']>=50) & (data['stroke']==0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160",
   "metadata": {},
   "source": [
    "#### Upon examining the dataset and generated counterfactuals, we observed several instances with **extremely high BMI values (â‰¥ 50)** that produce unrealistic or implausible predictions. Due to the dataset's high class imbalance, although higher BMI should generally increase stroke risk, the majority of these high-BMI instances have **stroke = 0**, which skews the modelâ€™s behavior. These instances can be considered **outliers**, and since they represent a small portion of the data, it is reasonable to remove them to improve the plausibility and reliability of subsequent analyses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161",
   "metadata": {},
   "source": [
    "### 5/\n",
    "\n",
    "**Original patient profile:**  \n",
    "\n",
    "| Gender | Age  | Hypertension | Heart Disease | Married | Work Type | Residence | Avg Glucose | BMI  | Smoking Status | Target |\n",
    "|--------|------|--------------|---------------|---------|-----------|-----------|-------------|------|----------------|--------|\n",
    "| Male   | 45.0 | 0            | 0             | Yes     | Govt_job  | Urban     | 55.47       | 19.8 | smokes         | 0      |\n",
    "\n",
    "**Counterfactuals generated:**  \n",
    "\n",
    "| Gender | Age  | Hypertension | Heart Disease | Married | Work Type | Residence | Avg Glucose | BMI  | Smoking Status | Target |\n",
    "|--------|------|--------------|---------------|---------|-----------|-----------|-------------|------|----------------|--------|\n",
    "| Female | 51.9 | 0            | 0             | Yes     | Private   | Urban     | 71.40       | 44.3 | smokes         | 1      |\n",
    "| Female | 80.8 | 0            | 0             | No      | Private   | Urban     | 71.40       | 28.4 | smokes         | 1      |\n",
    "| Female | 66.8 | 0            | 0             | Yes     | Private   | Urban     | 158.47      | 28.4 | smokes         | 1      |\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "1. **Target change:** All counterfactuals flip stroke prediction from 0 â†’ 1.  \n",
    "2. **Feature changes and plausibility:**  \n",
    "   - Gender changed from Male â†’ Female in all counterfactuals.  \n",
    "   - Age varies widely (45 â†’ 51.9, 80.8, 66.8).  \n",
    "   - Work type changed from Govt_job â†’ Private.  \n",
    "   - BMI is extreme in one counterfactual (44.3) and normal in others.  \n",
    "   - Avg glucose increased substantially in one counterfactual (55.47 â†’ 158.47).  \n",
    "   - Some changes (like large BMI jump or very high glucose) may be **clinically unrealistic**.  \n",
    "3. **Insight:**  \n",
    "   - The model is **very sensitive to gender**, which can flip the predicted risk.  \n",
    "\n",
    "### Actionable insight\n",
    "\n",
    "- Counterfactual analysis shows that **changing gender, glucose, or BMI** can alter predictions.  \n",
    "- For meaningful interpretation, **focus on plausible counterfactuals** within realistic clinical ranges.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162",
   "metadata": {},
   "source": [
    "## CEs for XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163",
   "metadata": {},
   "source": [
    "### 1/\n",
    "\n",
    "**Original patient profile:**  \n",
    "\n",
    "| Gender | Age  | Hypertension | Heart Disease | Married | Work Type | Residence | Avg Glucose | BMI  | Smoking Status | Target |\n",
    "|--------|------|--------------|---------------|---------|-----------|-----------|-------------|------|----------------|--------|\n",
    "| Male   | 45.0 | No           | No            | Yes     | Govt_job  | Urban     | 55.47       | 19.8 | smokes         | 0      |\n",
    "\n",
    "**Counterfactuals generated:**  \n",
    "\n",
    "| Gender | Age   | Hypertension | Heart Disease | Married | Work Type | Residence | Avg Glucose | BMI  | Smoking Status | Target |\n",
    "|--------|-------|--------------|---------------|---------|-----------|-----------|-------------|------|----------------|--------|\n",
    "| Male   | 60.2  | 0            | 0             | Yes     | Govt_job  | Urban     | 195.75      | 19.8 | smokes         | 1      |\n",
    "| Male   | 81.4  | 1            | 0             | Yes     | Govt_job  | Urban     | 199.75      | 19.8 | smokes         | 1      |\n",
    "| Female | 82.0  | 0            | 0             | Yes     | Govt_job  | Urban     | 152.70      | 19.8 | smokes         | 1      |\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "1. **Target change:** All counterfactuals flip the predicted stroke from 0 â†’ 1.  \n",
    "\n",
    "2. **Feature changes:**  \n",
    "   - **Age** increases substantially in all counterfactuals.  \n",
    "   - **Hypertension** changes in one counterfactual.  \n",
    "   - **Gender** changes in one counterfactual.  \n",
    "   - **Average glucose level** rises sharply in all counterfactuals.  \n",
    "   - Other features (BMI, marital status, work type, residence, smoking) remain unchanged.  \n",
    "\n",
    "3. **Unrealistic pattern:**  \n",
    "   - The counterfactual suggesting **gender change (Male â†’ Female)** may not be practically actionable.  But still the somehow the Female has more chance to have a stroke than Male\n",
    "   - At first glance, extremely high **average glucose levels (152.7â€“199.75)** are likely **clinically extreme** for someone with BMI 19.8 and age 45â€“82.  But, while we often associate high glucose (blood sugar) with higher body weight, it is a common misconception that a low or normal BMI \"protects\" you from metabolic issues. In fact, about 10% to 15% of people diagnosed with Type 2 diabetes are at a normal or low weight. This is often referred to as \"lean diabetes.\"\n",
    "\n",
    "### Actionable insight\n",
    "- This CEs suggest that to maintain this low-risk status, it is essential to prevent high glucose index and the hypertension. Essentially, the model suggests that while the patient is currently safe, spikes in blood sugar and blood pressure are the fastest routes to a high stroke risk classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164",
   "metadata": {},
   "source": [
    "### 2/\n",
    "\n",
    "### Original patient profile:\n",
    "\n",
    "| Gender | Age | Hypertension | Heart Disease | Married | Work Type | Residence | Avg Glucose | BMI  | Smoking Status | Target |\n",
    "|--------|-----|--------------|---------------|---------|-----------|-----------|-------------|------|----------------|--------|\n",
    "| Male | 58 | No | No | Yes | Private | Urban | 94.00 | 28.1 | Unknown | 1 |\n",
    "\n",
    "### Counterfactuals generated:\n",
    "\n",
    "| Gender | Age | Hypertension | Heart Disease | Married | Work Type | Residence | Avg Glucose | BMI  | Smoking Status | Target |\n",
    "|--------|-----|--------------|---------------|---------|-----------|-----------|-------------|------|----------------|--------|\n",
    "| Male | 55 | No | No | Yes | Self-employed | Urban | 93.67 | 29.3 | Unknown | 0 |\n",
    "| Male | 56 | No | No | Yes | Private | Urban | 96.93 | 25.0 | smokes | 0 |\n",
    "| Male | 57 | No | Yes | Yes | Self-employed | Urban | 92.82 | 27.8 | formerly smoked | 0 |\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "1. **Target change:**  \n",
    "   - All counterfactuals flip the prediction from **1 â†’ 0** (stroke â†’ non-stroke).\n",
    "\n",
    "2. **Feature changes:**  \n",
    "   - The most influential changes are:\n",
    "     - **Age decrease** (from 58 â†’ 55â€“57),\n",
    "     - Slight variations in **average glucose level**,\n",
    "     - Changes in **work type** and **smoking status**.\n",
    "   - One counterfactual introduces **heart disease**, yet the prediction still flips to non-stroke.\n",
    "\n",
    "3. **Unrealistic pattern:**  \n",
    "   - The model predicts **lower stroke risk even with heart disease present**, while small reductions in age dominate the outcome.\n",
    "   - Clinically, heart disease is a strong risk factor and should not be outweighed by such minor feature changes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165",
   "metadata": {},
   "source": [
    "### 3/\n",
    "\n",
    "**Original patient profile (idx:50):**\n",
    "\n",
    "| Gender | Age  | Hypertension | Heart Disease | Married | Work Type | Residence | Avg Glucose | BMI  | Smoking Status | Target |\n",
    "|--------|------|--------------|---------------|---------|-----------|-----------|-------------|------|----------------|--------|\n",
    "| Female | 57.0 | No           | No            | Yes     | Private   | Urban     | 95.40       | 19.5 | Unknown        | 1      |\n",
    "\n",
    "**Counterfactuals generated:**\n",
    "\n",
    "| Gender | Age  | Hypertension | Heart Disease | Married | Work Type | Residence | Avg Glucose | BMI  | Smoking Status | Target |\n",
    "|--------|------|--------------|---------------|---------|-----------|-----------|-------------|------|----------------|--------|\n",
    "| Female | 45.0 | 0            | 0             | Yes     | Private   | Urban     | 97.95       | 24.5 | Unknown        | 0      |\n",
    "| Female | 0.1  | 0            | 0             | Yes     | Private   | Urban     | 91.89       | 23.3 | Unknown        | 0      |\n",
    "| Female | 0.1  | 0            | 0             | Yes     | Private   | Urban     | 83.09       | 24.5 | Unknown        | 0      |\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "1. **Target change:**  \n",
    "   All counterfactuals flip the predicted stroke from 1 â†’ 0.  \n",
    "\n",
    "2. **Feature changes:**  \n",
    "   - The most significant changes are **age** and **BMI**, which decrease to much lower values.  \n",
    "   - Hypertension, heart disease, work type, residence, and marriage status remain unchanged.  \n",
    "   - Average glucose decreases slightly in some counterfactuals but remains relatively normal.  \n",
    "\n",
    "3. **Insight:**  \n",
    "   - The model heavily relies on **age** to predict stroke risk; extremely low age immediately flips the prediction to 0, even when BMI and glucose are elevated.  \n",
    "   - This highlights that the model may overemphasize age relative to other risk factors.  \n",
    "    \n",
    "\n",
    "### Actionable insight\n",
    "\n",
    "- Stroke risk in this scenario is driven primarily by **age**, with secondary influence from BMI and glucose.  \n",
    "- **Extremely young counterfactuals** (0.1 years) illustrate that the model is highly influtential by age which isnot helpful in clinical insight.(i realized we should exclude this but it's too late too change).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166",
   "metadata": {},
   "source": [
    "### 4/\n",
    "\n",
    "**Original patient profile (idx:100, FOr camparsion):**\n",
    "\n",
    "| Gender | Age  | Hypertension | Heart Disease | Married | Work Type | Residence | Avg Glucose | BMI  | Smoking Status | Target |\n",
    "|--------|------|--------------|---------------|---------|-----------|-----------|-------------|------|----------------|--------|\n",
    "| Male   | 58.0 | No           | No            | Yes     | Private   | Urban     | 94.0        | 28.1 | Unknown        | 1      |\n",
    "\n",
    "**Counterfactuals generated:**\n",
    "\n",
    "| Gender | Age   | Hypertension | Heart Disease | Married | Work Type | Residence | Avg Glucose | BMI  | Smoking Status | Target |\n",
    "|--------|-------|--------------|---------------|---------|-----------|-----------|-------------|------|----------------|--------|\n",
    "| Male   | 58.0  | 1            | 0             | Yes     | Private   | Urban     | 94.00       | 12.6 | Unknown        | 0      |\n",
    "| Male   | 22.2  | 0            | 0             | Yes     | Govt_job  | Urban     | 94.00       | 28.1 | Unknown        | 0      |\n",
    "| Male   | 58.0  | 0            | 0             | Yes     | Private   | Urban     | 65.92       | 28.1 | Unknown        | 0      |\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "1. **Target change:**  \n",
    "   All counterfactuals flip the predicted stroke from 1 â†’ 0.  \n",
    "\n",
    "2. **Feature changes:**  \n",
    "   - **Hypertension** is added in one counterfactual.  \n",
    "   - **Age** decreases dramatically in one counterfactual (58 â†’ 22.2).  \n",
    "   - **Average glucose** decreases in one counterfactual (94 â†’ 65.92).  \n",
    "   - **BMI** drops sharply in one counterfactual (28.1 â†’ 12.6).  \n",
    "   - Other features (gender, heart disease, marital status, work type, residence, smoking) remain unchanged.  \n",
    "\n",
    "3. **Insight:**  \n",
    "   - The model is highly sensitive to **age**, **BMI**, and **glucose** in predicting stroke risk.  \n",
    "   - In some cases, **adding hypertension paradoxically reduces risk**, which may reflect **data skew**, where non-stroke cases with hypertension are overrepresented.  \n",
    " \n",
    "### Actionable insight\n",
    "\n",
    "- Stroke risk is influenced by a combination of (BMI, glucose) and age.  \n",
    "- Clinically actionable interventions include:  \n",
    "  - **Weight management** to reduce elevated BMI to healthy ranges  \n",
    "  - **Glucose monitoring and control**, even if mildly elevated  \n",
    "  - **Stress or lifestyle management** associated with work type or other contextual factors.  \n",
    "- **Data skew caution:** The model may produce counterintuitive predictions (e.g., adding hypertension does not increase stroke risk) because most non-stroke cases in the dataset do not have heart disease.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167",
   "metadata": {},
   "source": [
    "## CEs for Catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168",
   "metadata": {},
   "source": [
    "### 1/\n",
    "\n",
    "**Original patient profile :**\n",
    "\n",
    "| Gender | Age  | Hypertension | Heart Disease | Married | Work Type | Residence | Avg Glucose | BMI  | Smoking Status | Target |\n",
    "|--------|------|--------------|---------------|---------|-----------|-----------|-------------|------|----------------|--------|\n",
    "| Male   | 45.0 | No           | No            | Yes     | Govt_job  | Urban     | 55.47       | 19.8 | smokes         | 0      |\n",
    "\n",
    "**Counterfactuals generated:**\n",
    "\n",
    "| Gender | Age  | Hypertension | Heart Disease | Married | Work Type       | Residence | Avg Glucose | BMI  | Smoking Status | Target |\n",
    "|--------|------|--------------|---------------|---------|-----------------|-----------|-------------|------|----------------|--------|\n",
    "| Female | 45.0 | 0            | 0             | Yes     | Private         | Urban     | 72.65       | 25.6 | Unknown        | 1      |\n",
    "| Female | 46.0 | 0            | 0             | Yes     | Self-employed   | Urban     | 71.12       | 27.3 | never smoked  | 1      |\n",
    "| Female | 40.0 | 0            | 0             | Yes     | Private         | Urban     | 71.20       | 27.1 | never smoked  | 1      |\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "1. **Target change:**  \n",
    "   All counterfactuals flip the predicted stroke from 0 â†’ 1.  \n",
    "\n",
    "2. **Feature changes:**  \n",
    "   - The most notable changes are **gender**, **BMI**, **average glucose level**, and **work type**.  \n",
    "   - **Age remains approximately constant**, indicating that the prediction change is not driven by aging in this case.  \n",
    "   - Hypertension and heart disease remain unchanged across all counterfactuals.  \n",
    "\n",
    "3. **Insight:**  \n",
    "   - The model assigns a higher stroke risk to **female profiles** compared to the original male profile, even at similar ages.  \n",
    "   - Moderate increases in **BMI** and **average glucose**, while still within relatively normal ranges, are sufficient to flip the prediction.  \n",
    "   - This suggests that the model captures **gender-dependent risk patterns** and is sensitive to relatively small metabolic changes when age is held constant.  \n",
    "\n",
    "### Actionable insight\n",
    "\n",
    "- These counterfactuals indicate that stroke risk is driven by a **combination of metabolic factors (BMI, glucose level)** and **lifestyle-related factors**, rather than age alone.  \n",
    "- The change in **work type** (e.g., Govt_job â†’ Private / Self-employed) suggests that **stress-related factors**â€”such as job pressure, workload, or employment instabilityâ€”may play a role in increasing predicted stroke risk.  \n",
    "- From a practical perspective, actionable recommendations should focus on **modifiable risk factors**, including:\n",
    "  - Maintaining healthy **body weight (BMI)**  \n",
    "  - Monitoring and controlling **blood glucose levels**, even when they are not in an extreme range  \n",
    "  - Managing **work-related stress** through workload regulation, stress management strategies, or lifestyle adjustments  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169",
   "metadata": {},
   "source": [
    "### 2/\n",
    "\n",
    "**Original patient profile :**\n",
    "\n",
    "| Gender | Age  | Hypertension | Heart Disease | Married | Work Type | Residence | Avg Glucose | BMI  | Smoking Status | Target |\n",
    "|--------|------|--------------|---------------|---------|-----------|-----------|-------------|------|----------------|--------|\n",
    "| Female | 69.0 | No           | Yes           | No      | Govt_job  | Urban     | 202.38      | 34.6 | Unknown        | 1      |\n",
    "\n",
    "**Counterfactuals generated:**\n",
    "\n",
    "| Gender | Age  | Hypertension | Heart Disease | Married | Work Type | Residence | Avg Glucose | BMI  | Smoking Status | Target |\n",
    "|--------|------|--------------|---------------|---------|-----------|-----------|-------------|------|----------------|--------|\n",
    "| Female | 50.0 | 0            | 1             | Yes     | Private   | Urban     | 193.80      | 26.4 | never smoked  | 0      |\n",
    "| Female | 52.0 | 0            | 0             | Yes     | Private   | Rural     | 200.46      | 25.0 | Unknown        | 0      |\n",
    "| Female | 53.0 | 1            | 1             | Yes     | Private   | Urban     | 196.25      | 24.9 | smokes        | 0      |\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "1. **Target change:**  \n",
    "   All counterfactuals flip the predicted stroke from 1 â†’ 0.  \n",
    "\n",
    "2. **Feature changes:**  \n",
    "   - The dominant change across all counterfactuals is **age**, which decreases by approximately 16â€“19 years.  \n",
    "   - **Marriage status** and **work type** change consistently from *unmarried Govt_job* to *married Private employment*.  \n",
    "   - **BMI decreases substantially**, moving from an obese range (34.6) to near-normal values (24.9â€“26.4).  \n",
    "   - **Heart disease and hypertension** vary across counterfactuals, indicating secondary influence.  \n",
    "   - Average glucose remains **extremely high** in all cases.  \n",
    "\n",
    "3. **Insight:**  \n",
    "   - The model predicts that a combination of **younger age**, **lower BMI**, and **changes in work context** can outweigh the effect of persistently high glucose levels.  \n",
    "   - The shift from *Govt_job* to *Private employment* may implicitly capture **stress-related or lifestyle differences** that influence stroke risk.  \n",
    "   - In some cases, active heart disease, the model assigns lower stroke risk to these profiles\n",
    "   \n",
    "### Actionable insight\n",
    "\n",
    "- These counterfactuals suggest that **stroke risk is shaped by BMI and lifestyle-related factors**, rather than glucose levels alone.  \n",
    "- Concrete, actionable recommendations derived from this analysis include:  \n",
    "  - **Weight management** to reduce BMI from obese to healthy ranges  \n",
    "  - **Stress-aware lifestyle**, particularly related to occupational conditions and work-related pressure  \n",
    "\n",
    "- **Model caution:** In some counterfactuals, profiles with active heart disease are assigned lower stroke risk. This likely reflects a **skew in the data**, where a portion of non-stroke cases have heart disease. As a result, the model has learned a **spurious correlation**: â€œheart disease and hypertenstion â†’ lower stroke probability.â€  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170",
   "metadata": {},
   "source": [
    "### 3/\n",
    "\n",
    "**Original patient profile (idx:50, For Comparasion):**\n",
    "\n",
    "| Gender | Age  | Hypertension | Heart Disease | Married | Work Type | Residence | Avg Glucose | BMI  | Smoking Status | Target |\n",
    "|--------|------|--------------|---------------|---------|-----------|-----------|-------------|------|----------------|--------|\n",
    "| Female | 57.0 | No           | No            | Yes     | Private   | Urban     | 95.40       | 19.5 | Unknown        | 1      |\n",
    "\n",
    "**Counterfactuals generated:**\n",
    "\n",
    "| Gender | Age  | Hypertension | Heart Disease | Married | Work Type | Residence | Avg Glucose | BMI  | Smoking Status | Target |\n",
    "|--------|------|--------------|---------------|---------|-----------|-----------|-------------|------|----------------|--------|\n",
    "| Female | 58.0 | 0            | 0             | Yes     | Private   | Urban     | 96.21       | 23.5 | never smoked  | 0      |\n",
    "| Male   | 57.0 | 0            | 0             | Yes     | Private   | Rural     | 92.59       | 24.2 | Unknown        | 0      |\n",
    "| Female | 54.0 | 0            | 0             | Yes     | Private   | Urban     | 99.83       | 22.7 | formerly smoked | 0      |\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "1. **Target change:**  \n",
    "   All counterfactuals flip the predicted stroke from 1 â†’ 0.  \n",
    "\n",
    "2. **Feature changes:**  \n",
    "   - **BMI** increases slightly in two counterfactuals (19.5 â†’ 22.7â€“23.5).  \n",
    "   - **Age** varies slightly (54â€“58) in counterfactuals.  \n",
    "   - **Gender** changes in one case (Female â†’ Male).  \n",
    "   - Minor variations in **average glucose** and **smoking status** are observed.  \n",
    "   - Hypertension and heart disease remain unchanged.  \n",
    "\n",
    "3. **Insight:**  \n",
    "   - The model is sensitive to **BMI, age, and gender**  \n",
    "   - The change from Female â†’ Male suggests that Male has more chance to have stroke than Female(Checked by real clinical knowledge)  \n",
    "   - Slight increases in BMI and changes in glucose show that the model considers even moderate variations in metabolic factors when determining stroke risk.  \n",
    "\n",
    "### Actionable insight\n",
    "\n",
    "- Stroke risk is influenced by (BMI, glucose), as well as lifestyle or stress-related context implied by work type and residence.  \n",
    "- Concrete interventions could include:  \n",
    "  - **Healthy weight maintenance** to optimize BMI  \n",
    "  - **Monitoring blood glucose** even if values appear normal  \n",
    "  - **Managing stress** associated with work or daily routines  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171",
   "metadata": {},
   "source": [
    "### 4/\n",
    "\n",
    "**Original patient profile (idx:100, For Comaparasion):**\n",
    "\n",
    "| Gender | Age  | Hypertension | Heart Disease | Married | Work Type | Residence | Avg Glucose | BMI  | Smoking Status | Target |\n",
    "|--------|------|--------------|---------------|---------|-----------|-----------|-------------|------|----------------|--------|\n",
    "| Male   | 58.0 | No           | No            | Yes     | Private   | Urban     | 94.0        | 28.1 | Unknown        | 1      |\n",
    "\n",
    "**Counterfactuals generated:**\n",
    "\n",
    "| Gender | Age   | Hypertension | Heart Disease | Married | Work Type     | Residence | Avg Glucose | BMI  | Smoking Status   | Target |\n",
    "|--------|-------|--------------|---------------|---------|---------------|-----------|-------------|------|-----------------|--------|\n",
    "| Male   | 55.0  | 0            | 0             | Yes     | Self-employed | Urban     | 93.67       | 29.3 | Unknown         | 0      |\n",
    "| Male   | 56.0  | 0            | 0             | Yes     | Private       | Urban     | 96.93       | 25.0 | smokes          | 0      |\n",
    "| Male   | 57.0  | 0            | 1             | Yes     | Self-employed | Urban     | 92.82       | 27.8 | formerly smoked | 0      |\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "1. **Target change:**  \n",
    "   All counterfactuals flip the predicted stroke from 1 â†’ 0.  \n",
    "\n",
    "2. **Feature changes:**  \n",
    "   - **Age** decreases slightly in some counterfactuals (58 â†’ 55â€“57).  \n",
    "   - **BMI** varies moderately (25.0â€“29.3).  \n",
    "   - **Heart disease** changes in one counterfactual, showing secondary influence.  \n",
    "   - **Work type** changes (Private â†’ Self-employed) in two counterfactuals, suggesting lifestyle or stress-related factors affect stroke prediction.  \n",
    "   - Average glucose remains roughly constant (92â€“97), and other features remain unchanged.  \n",
    "\n",
    "3. **Insight:**  \n",
    "   - The model considers **age, BMI, and work type** as key factors for predicting stroke risk in this profile.  \n",
    "   - Subtle differences in (BMI) and **contextual features** (work type) are sufficient to flip the predicted outcome.  \n",
    "   - The heart disease change in one counterfactual illustrates that the data is skewed to no stroke .\n",
    "\n",
    "### Actionable insight\n",
    "\n",
    "- Stroke risk is influenced by (BMI) and **stress-related lifestyle factors** (work type).  \n",
    "- Recommendations for this patient may include:  \n",
    "  - **Weight management** to optimize BMI  \n",
    "  - **Monitoring and controlling glucose levels**, even if moderately elevated  \n",
    "  - **Stress reduction and lifestyle interventions**, particularly related to employment type or work conditions  \n",
    "- **Data skew caution:** The model may assign lower stroke risk even when heart disease is present because most non-stroke cases in the training data do not have heart disease.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172",
   "metadata": {},
   "source": [
    "## Summary of Counterfactual Explanations (CEs)\n",
    "\n",
    "From analyzing multiple patient profiles and their counterfactuals, several clear patterns emerge regarding how the model predicts stroke risk:\n",
    "\n",
    "1. **Age dominates the model predictions**  \n",
    "   - Across nearly all CEs, **age is the most influential feature**.  \n",
    "   - While this drives target changes effectively, it is **not clinically actionable**, as we cannot modify a patientâ€™s age.  \n",
    "\n",
    "2. **Metabolic factors (BMI and average glucose) are predictive when age is fixed**  \n",
    "   - When age is held constant in counterfactual generation, the model often adjusts **BMI** or **average glucose** to flip predictions.  \n",
    "   - Keeping these features within **normal or lower ranges** decreases the predicted stroke risk.  \n",
    "   - These are **clinically actionable interventions**, such as weight management and glucose control.  \n",
    "\n",
    "3. **Stress-related or lifestyle factors**  \n",
    "   - Some CEs suggest that changing **work type** (e.g., Self-employed â†’ Private) or **residence type** can reduce stroke risk.  \n",
    "   - This aligns with real-world evidence: **stress and lifestyle influence cardiovascular health**.  \n",
    "\n",
    "4. **Marriage appears protective**  \n",
    "   - Interestingly, many CEs show that **being married** slightly lowers predicted stroke risk.  \n",
    "   - A â€œfun insightâ€: **the power of love may be real**, at least according to the modelâ€™s learned patterns.  \n",
    "\n",
    "5. **Challenges with actionable feature constraints**  \n",
    "   - If **age and gender are fixed** (to focus on modifiable features), DiCE must rely on other variables like **BMI, glucose, or smoking** to flip the prediction.  \n",
    "   - If these features have **low feature importance**, the Dice may **fail to generate a feasible counterfactual**, resulting in an **underconstrained problem**.  \n",
    "\n",
    "6. **Detecting outliers**  \n",
    "   - CEs are powerful for identifying **rare outliers or extreme feature combinations** that may not be obvious in standard analysis.  \n",
    "   - This helps uncover **data issues or rare risk patterns** that could be clinically relevant.  \n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- The **main actionable insights** from CEs are:  \n",
    "  1. **Maintain healthy BMI**  \n",
    "  2. **Control glucose levels**  \n",
    "  3. **Address stress-related lifestyle factors** (work, residence, daily habits)  \n",
    "\n",
    "- **Immutable features** like age and gender should generally be fixed to focus on clinically relevant interventions.  \n",
    "- In small datasets where **age is extremely predictive**, the DiCE may struggle to find feasible counterfactuals if age is fixed.  \n",
    "- Overall, CEs provide a **unique window into the modelâ€™s learned patterns**, offering both **actionable clinical guidance** and the ability to **detect outliers or unusual cases**.  \n",
    "\n",
    "> **Conclusion:** Counterfactual explanations transform complex model behavior into **practical insights**, highlighting which features can be realistically modified to reduce stroke risk, while also revealing hidden patterns like stress, marriage, or outlier profiles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173",
   "metadata": {},
   "source": [
    "# Partial Dependence Plots & Individual Conditional Expectation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "%pip install catboost\n",
    "\n",
    "features = [\"age\", \"avg_glucose_level\", \"bmi\"]\n",
    "\n",
    "PartialDependenceDisplay.from_estimator(\n",
    "    estimator=rf,\n",
    "    X=X_test,\n",
    "    features=features,\n",
    "    kind=\"average\",\n",
    "    grid_resolution=50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175",
   "metadata": {},
   "outputs": [],
   "source": [
    "PartialDependenceDisplay.from_estimator(\n",
    "    estimator=rf,\n",
    "    X=X_test,\n",
    "    features=features,\n",
    "    kind=\"both\",\n",
    "    subsample=100,\n",
    "    grid_resolution=50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "features = [\"age\", \"avg_glucose_level\", \"bmi\"]\n",
    "\n",
    "PartialDependenceDisplay.from_estimator(\n",
    "    estimator=xgb,          # æˆ– xgb\n",
    "    X=X_test,\n",
    "    features=features,\n",
    "    kind=\"average\",         # PDP\n",
    "    grid_resolution=50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177",
   "metadata": {},
   "outputs": [],
   "source": [
    "PartialDependenceDisplay.from_estimator(\n",
    "    estimator=xgb,\n",
    "    X=X_test,\n",
    "    features=features,\n",
    "    kind=\"both\",\n",
    "    subsample=100,\n",
    "    grid_resolution=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "\n",
    "required_cols = {\"gender\", \"age\", \"avg_glucose_level\", \"bmi\"}\n",
    "missing = required_cols - set(X_test.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"X_test is missing required columns: {missing}\")\n",
    "\n",
    "\n",
    "BMI_THRESHOLD = 30.0   # obesity cutoff (common choice)\n",
    "high_bmi = X_test[X_test[\"bmi\"] >= BMI_THRESHOLD].copy()\n",
    "\n",
    "print(f\"[Subgroup] High BMI (bmi >= {BMI_THRESHOLD}) size: {len(high_bmi)} / {len(X_test)}\")\n",
    "\n",
    "if len(high_bmi) < 50:\n",
    "    print(\"Warning: High BMI subgroup is small; ICE/PDP may be noisy.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_bmi_sorted = high_bmi.sort_values(\"avg_glucose_level\")\n",
    "low_glucose_person = high_bmi_sorted.iloc[[0]].copy()\n",
    "high_glucose_person = high_bmi_sorted.iloc[[-1]].copy()\n",
    "\n",
    "print(\"\\n[Representative Individuals within High BMI]\")\n",
    "print(\"Low glucose person:\")\n",
    "display(low_glucose_person)\n",
    "print(\"High glucose person:\")\n",
    "display(high_glucose_person)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_pdp_ice_for_subgroup(estimator, X_subgroup, feature=\"avg_glucose_level\",\n",
    "                             model_name=\"Model\", subsample=100, grid_resolution=50):\n",
    "    \"\"\"\n",
    "    Plots PDP + ICE for a single feature in a given subgroup.\n",
    "    - PDP (average effect): thick line\n",
    "    - ICE (individual effects): many thin lines (subsampled)\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(8, 5))\n",
    "    PartialDependenceDisplay.from_estimator(\n",
    "        estimator=estimator,\n",
    "        X=X_subgroup,\n",
    "        features=[feature],\n",
    "        kind=\"both\",                 # PDP + ICE\n",
    "        subsample=subsample,         # number of ICE lines\n",
    "        grid_resolution=grid_resolution,\n",
    "        random_state=42\n",
    "    )\n",
    "    plt.title(f\"{model_name}: PDP + ICE for '{feature}' (High BMI subgroup)\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_pdp_ice_for_subgroup(\n",
    "    estimator=rf,\n",
    "    X_subgroup=high_bmi,\n",
    "    feature=\"avg_glucose_level\",\n",
    "    model_name=\"Random Forest\",\n",
    "    subsample=100,\n",
    "    grid_resolution=50\n",
    ")\n",
    "\n",
    "\n",
    "plot_pdp_ice_for_subgroup(\n",
    "    estimator=xgb,\n",
    "    X_subgroup=high_bmi,\n",
    "    feature=\"avg_glucose_level\",\n",
    "    model_name=\"XGBoost\",\n",
    "    subsample=100,\n",
    "    grid_resolution=50\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_individual_ice(estimator, person_row: pd.DataFrame, feature: str, grid: np.ndarray):\n",
    "    \"\"\"\n",
    "    For a single individual (one-row DataFrame), vary `feature` across `grid`,\n",
    "    predict proba for class 1 (stroke), return probabilities.\n",
    "    \"\"\"\n",
    "    if person_row.shape[0] != 1:\n",
    "        raise ValueError(\"person_row must be a one-row DataFrame.\")\n",
    "    X_tmp = pd.concat([person_row] * len(grid), ignore_index=True)\n",
    "    X_tmp[feature] = grid\n",
    "\n",
    "\n",
    "    if hasattr(estimator, \"predict_proba\"):\n",
    "        proba = estimator.predict_proba(X_tmp)[:, 1]\n",
    "    else:\n",
    "\n",
    "        if hasattr(estimator, \"decision_function\"):\n",
    "            scores = estimator.decision_function(X_tmp)\n",
    "\n",
    "            proba = 1 / (1 + np.exp(-scores))\n",
    "        else:\n",
    "            proba = estimator.predict(X_tmp)\n",
    "\n",
    "    return proba\n",
    "\n",
    "def plot_two_individual_ice(estimator, person_a, person_b, feature=\"avg_glucose_level\",\n",
    "                            model_name=\"Model\", grid_resolution=80):\n",
    "    \"\"\"\n",
    "    Plot ICE curves for two individuals by varying `feature`.\n",
    "    Grid is constructed from subgroup min/max for interpretability.\n",
    "    \"\"\"\n",
    "\n",
    "    fmin = float(high_bmi[feature].min())\n",
    "    fmax = float(high_bmi[feature].max())\n",
    "    grid = np.linspace(fmin, fmax, grid_resolution)\n",
    "\n",
    "    proba_a = compute_individual_ice(estimator, person_a, feature, grid)\n",
    "    proba_b = compute_individual_ice(estimator, person_b, feature, grid)\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(grid, proba_a, label=\"High BMI + LOW glucose representative\")\n",
    "    plt.plot(grid, proba_b, label=\"High BMI + HIGH glucose representative\")\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"Predicted P(stroke=1)\")\n",
    "    plt.title(f\"{model_name}: Individual ICE curves for '{feature}' (High BMI subgroup)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_two_individual_ice(\n",
    "    estimator=rf,\n",
    "    person_a=low_glucose_person,\n",
    "    person_b=high_glucose_person,\n",
    "    feature=\"avg_glucose_level\",\n",
    "    model_name=\"Random Forest\",\n",
    "    grid_resolution=80\n",
    ")\n",
    "\n",
    "\n",
    "plot_two_individual_ice(\n",
    "    estimator=xgb,\n",
    "    person_a=low_glucose_person,\n",
    "    person_b=high_glucose_person,\n",
    "    feature=\"avg_glucose_level\",\n",
    "    model_name=\"XGBoost\",\n",
    "    grid_resolution=80\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n[High BMI subgroup summary]\")\n",
    "print(high_bmi[[\"age\", \"avg_glucose_level\", \"bmi\"]].describe())\n",
    "\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.hist(high_bmi[\"avg_glucose_level\"], bins=30)\n",
    "plt.title(\"avg_glucose_level distribution (High BMI subgroup)\")\n",
    "plt.xlabel(\"avg_glucose_level\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "\n",
    "\n",
    "required_cols = {\"gender\", \"age\", \"avg_glucose_level\", \"bmi\"}\n",
    "missing = required_cols - set(X_test.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"X_test is missing required columns: {missing}\")\n",
    "\n",
    "\n",
    "high_bmi = X_test[X_test[\"bmi\"] >= BMI_THRESHOLD].copy()\n",
    "\n",
    "print(f\"[Subgroup] High BMI (bmi >= {BMI_THRESHOLD}) size: {len(high_bmi)} / {len(X_test)}\")\n",
    "\n",
    "if len(high_bmi) < 50:\n",
    "    print(\"Warning: High BMI subgroup is small; ICE/PDP may be noisy.\")\n",
    "\n",
    "\n",
    "high_bmi_sorted = high_bmi.sort_values(\"avg_glucose_level\")\n",
    "low_glucose_person = high_bmi_sorted.iloc[[0]].copy()\n",
    "high_glucose_person = high_bmi_sorted.iloc[[-1]].copy()\n",
    "\n",
    "print(\"\\n[Representative Individuals within High BMI]\")\n",
    "print(\"Low glucose person:\")\n",
    "display(low_glucose_person)\n",
    "print(\"High glucose person:\")\n",
    "display(high_glucose_person)\n",
    "\n",
    "\n",
    "def plot_pdp_ice_for_subgroup(estimator, X_subgroup, feature=\"avg_glucose_level\",\n",
    "                             model_name=\"Model\", subsample=100, grid_resolution=50):\n",
    "    \"\"\"\n",
    "    Plots PDP + ICE for a single feature in a given subgroup.\n",
    "    - PDP (average effect): thick line\n",
    "    - ICE (individual effects): many thin lines (subsampled)\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(8, 5))\n",
    "    PartialDependenceDisplay.from_estimator(\n",
    "        estimator=estimator,\n",
    "        X=X_subgroup,\n",
    "        features=[feature],\n",
    "        kind=\"both\",\n",
    "        subsample=subsample,\n",
    "        grid_resolution=grid_resolution,\n",
    "        random_state=42\n",
    "    )\n",
    "    plt.title(f\"{model_name}: PDP + ICE for '{feature}' (High BMI subgroup)\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_pdp_ice_for_subgroup(\n",
    "    estimator=rf,\n",
    "    X_subgroup=high_bmi,\n",
    "    feature=\"avg_glucose_level\",\n",
    "    model_name=\"Random Forest\",\n",
    "    subsample=50,\n",
    "    grid_resolution=50\n",
    ")\n",
    "\n",
    "\n",
    "plot_pdp_ice_for_subgroup(\n",
    "    estimator=xgb,\n",
    "    X_subgroup=high_bmi,\n",
    "    feature=\"avg_glucose_level\",\n",
    "    model_name=\"XGBoost\",\n",
    "    subsample=50,\n",
    "    grid_resolution=50\n",
    ")\n",
    "\n",
    "\n",
    "def compute_individual_ice(estimator, person_row: pd.DataFrame, feature: str, grid: np.ndarray):\n",
    "    \"\"\"\n",
    "    For a single individual (one-row DataFrame), vary `feature` across `grid`,\n",
    "    predict proba for class 1 (stroke), return probabilities.\n",
    "    \"\"\"\n",
    "    if person_row.shape[0] != 1:\n",
    "        raise ValueError(\"person_row must be a one-row DataFrame.\")\n",
    "    X_tmp = pd.concat([person_row] * len(grid), ignore_index=True)\n",
    "    X_tmp[feature] = grid\n",
    "\n",
    "    # Predict probability of class 1 if available; otherwise fall back to predict()\n",
    "    if hasattr(estimator, \"predict_proba\"):\n",
    "        proba = estimator.predict_proba(X_tmp)[:, 1]\n",
    "    else:\n",
    "        # Some estimators may not implement predict_proba; use decision_function or predict\n",
    "        if hasattr(estimator, \"decision_function\"):\n",
    "            scores = estimator.decision_function(X_tmp)\n",
    "            # Convert scores to pseudo-prob via sigmoid (rough)\n",
    "            proba = 1 / (1 + np.exp(-scores))\n",
    "        else:\n",
    "            proba = estimator.predict(X_tmp)\n",
    "\n",
    "    return proba\n",
    "\n",
    "def plot_two_individual_ice(estimator, person_a, person_b, feature=\"avg_glucose_level\",\n",
    "                            model_name=\"Model\", grid_resolution=80):\n",
    "    \"\"\"\n",
    "    Plot ICE curves for two individuals by varying `feature`.\n",
    "    Grid is constructed from subgroup min/max for interpretability.\n",
    "    \"\"\"\n",
    "    # Build grid from high_bmi subgroup range for this feature\n",
    "    fmin = float(high_bmi[feature].min())\n",
    "    fmax = float(high_bmi[feature].max())\n",
    "    grid = np.linspace(fmin, fmax, grid_resolution)\n",
    "\n",
    "    proba_a = compute_individual_ice(estimator, person_a, feature, grid)\n",
    "    proba_b = compute_individual_ice(estimator, person_b, feature, grid)\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(grid, proba_a, label=\"High BMI + LOW glucose representative\")\n",
    "    plt.plot(grid, proba_b, label=\"High BMI + HIGH glucose representative\")\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"Predicted P(stroke=1)\")\n",
    "    plt.title(f\"{model_name}: Individual ICE curves for '{feature}' (High BMI subgroup)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Individual ICE curves for Random Forest\n",
    "plot_two_individual_ice(\n",
    "    estimator=rf,\n",
    "    person_a=low_glucose_person,\n",
    "    person_b=high_glucose_person,\n",
    "    feature=\"avg_glucose_level\",\n",
    "    model_name=\"Random Forest\",\n",
    "    grid_resolution=80\n",
    ")\n",
    "\n",
    "# Individual ICE curves for XGBoost\n",
    "plot_two_individual_ice(\n",
    "    estimator=xgb,\n",
    "    person_a=low_glucose_person,\n",
    "    person_b=high_glucose_person,\n",
    "    feature=\"avg_glucose_level\",\n",
    "    model_name=\"XGBoost\",\n",
    "    grid_resolution=80\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 5) (Optional) Quick subgroup stats to help you describe the subgroup\n",
    "# -----------------------------\n",
    "print(\"\\n[High BMI subgroup summary]\")\n",
    "print(high_bmi[[\"age\", \"avg_glucose_level\", \"bmi\"]].describe())\n",
    "\n",
    "# (Optional) If you want to see the distribution of glucose within high BMI\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.hist(high_bmi[\"avg_glucose_level\"], bins=30)\n",
    "plt.title(\"avg_glucose_level distribution (High BMI subgroup)\")\n",
    "plt.xlabel(\"avg_glucose_level\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184",
   "metadata": {},
   "outputs": [],
   "source": [
    "BMI_THRESHOLD = 30.0\n",
    "\n",
    "high_bmi = X_test[X_test[\"bmi\"] >= BMI_THRESHOLD].copy()\n",
    "\n",
    "high_bmi_female = high_bmi[high_bmi[\"gender\"] == 0]\n",
    "high_bmi_male   = high_bmi[high_bmi[\"gender\"] == 1]\n",
    "\n",
    "print(\"High BMI female:\", len(high_bmi_female))\n",
    "print(\"High BMI male:\", len(high_bmi_male))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_gender_specific_pdp_ice(estimator, X_subgroup, gender_name,\n",
    "                                 subsample=20, grid_resolution=50):\n",
    "    PartialDependenceDisplay.from_estimator(\n",
    "        estimator=estimator,\n",
    "        X=X_subgroup,\n",
    "        features=[\"avg_glucose_level\"],\n",
    "        kind=\"both\",                 # PDP + ICE\n",
    "        subsample=subsample,\n",
    "        grid_resolution=grid_resolution,\n",
    "        random_state=42\n",
    "    )\n",
    "    plt.title(f\"{gender_name} | High BMI | avg_glucose_level\")\n",
    "    plt.show()\n",
    "\n",
    "# Random Forest\n",
    "plot_gender_specific_pdp_ice(\n",
    "    estimator=rf,\n",
    "    X_subgroup=high_bmi_female,\n",
    "    gender_name=\"Female (High BMI)\",\n",
    "    subsample=50\n",
    ")\n",
    "\n",
    "plot_gender_specific_pdp_ice(\n",
    "    estimator=rf,\n",
    "    X_subgroup=high_bmi_male,\n",
    "    gender_name=\"Male (High BMI)\",\n",
    "    subsample=50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "plot_gender_specific_pdp_ice(\n",
    "    estimator=xgb,\n",
    "    X_subgroup=high_bmi_female,\n",
    "    gender_name=\"Female (High BMI)\",\n",
    "    subsample=50\n",
    ")\n",
    "\n",
    "plot_gender_specific_pdp_ice(\n",
    "    estimator=xgb,\n",
    "    X_subgroup=high_bmi_male,\n",
    "    gender_name=\"Male (High BMI)\",\n",
    "    subsample=50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "BMI_THRESHOLD = 30.0\n",
    "high_bmi = X_test[X_test[\"bmi\"] >= BMI_THRESHOLD].copy()\n",
    "\n",
    "age_bins = [0, 40, 60, 120]\n",
    "age_labels = [\"<40\", \"40-60\", \">=60\"]\n",
    "high_bmi[\"age_group\"] = pd.cut(high_bmi[\"age\"], bins=age_bins, labels=age_labels, right=False)\n",
    "\n",
    "print(\"High BMI size:\", len(high_bmi))\n",
    "print(high_bmi[\"age_group\"].value_counts(dropna=False))\n",
    "print(high_bmi[\"gender\"].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_map = {0: \"Female\", 1: \"Male\"}\n",
    "for g in sorted(high_bmi[\"gender\"].dropna().unique()):\n",
    "    for ag in age_labels:\n",
    "        X_sub = high_bmi[(high_bmi[\"gender\"] == g) & (high_bmi[\"age_group\"] == ag)]\n",
    "        plot_glucose_pdp_ice(\n",
    "            estimator=rf,\n",
    "            X_sub=X_sub,\n",
    "            title=f\"RF | High BMI | {gender_map.get(g, g)} | Age {ag}\",\n",
    "            subsample=20\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_glucose_pdp_ice(estimator, X_sub, title, subsample=20, grid_resolution=50):\n",
    "    if len(X_sub) < 20:\n",
    "        print(f\"Skip (too small): {title} | n={len(X_sub)}\")\n",
    "        return\n",
    "    \n",
    "    X_sub_model = X_sub.drop(columns=[\"age_group\"], errors=\"ignore\")\n",
    "\n",
    "    PartialDependenceDisplay.from_estimator(\n",
    "        estimator=estimator,\n",
    "        X=X_sub_model,\n",
    "        features=[\"avg_glucose_level\"],\n",
    "        kind=\"both\",\n",
    "        subsample=subsample,\n",
    "        grid_resolution=grid_resolution,\n",
    "        random_state=42\n",
    "    )\n",
    "    plt.title(f\"{title} | n={len(X_sub)}\")\n",
    "    plt.show()\n",
    "print(\"len(X_sub) =\", len(X_sub))\n",
    "print(\"columns =\", list(X_sub.columns)[:15], \"...\")\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "plot_glucose_pdp_ice(rf, X_sub, \"debug\", subsample=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190",
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in sorted(high_bmi[\"gender\"].dropna().unique()):\n",
    "    for ag in age_labels:\n",
    "        X_sub = high_bmi[(high_bmi[\"gender\"] == g) & (high_bmi[\"age_group\"] == ag)]\n",
    "        plot_glucose_pdp_ice(\n",
    "            estimator=xgb,\n",
    "            X_sub=X_sub,\n",
    "            title=f\"XGB | High BMI | {gender_map.get(g, g)} | Age {ag}\",\n",
    "            subsample=20\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
